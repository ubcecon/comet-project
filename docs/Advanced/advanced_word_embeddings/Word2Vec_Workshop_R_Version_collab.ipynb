{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCBDBEH7h6xz"
      },
      "source": [
        "# Introduction to Word Embeddings and Language Models\n",
        "\n",
        "_R Version_ for Google Collab\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/word_embedding_cover_art.png?raw=1\" width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "_This notebook was prepared by Laura Nelson in collaboration with [UBC COMET](https://comet.arts.ubc.ca/) team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBj5Ho0Uh6x4"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "1. Some familiarity programming in R\n",
        "2. Some familarity with natural language processing\n",
        "3. No computational text experience necessary!\n",
        "\n",
        "## Learning outcomes\n",
        "\n",
        "In the notebook you will\n",
        "\n",
        "1. Familiarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\n",
        "1. Import and pre-process a textual dataset for use in word embedding\n",
        "1. Use word2vec to build a simple language model for examining patterns and biases textual datasets\n",
        "1. Identify and select methods for saving and loading models\n",
        "1. Use critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models\n",
        "\n",
        "## Outline\n",
        "\n",
        "The goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\n",
        "\n",
        "In particular, this lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt's blog post, [Rejecting the Gender Binary](\"http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html).\n",
        "\n",
        "The primary corpus we will use consists of the <a href=\"http://txtlab.org/?p=601\">150 English-language novels</a> made available by the <em>.txtLab</em> at McGill University. We also look at a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNAQBzBZh6x5"
      },
      "source": [
        "## Key Terms\n",
        "Before we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.\n",
        "</div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/ai_key_terms.png?raw=1\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**Artificial Intelligence (AI):** this term is a broad category that includes the study and development of computer systems that can copy intelligent human behaviour (adapted from [_Oxford Learners Dictionary_](https://www.oxfordlearnersdictionaries.com/definition/english/ai#:~:text=%2F%CB%8Ce%C9%AA%20%CB%88a%C9%AA%2F-,%2F%CB%8Ce%C9%AA%20%CB%88a%C9%AA%2F,way%20a%20human%20brain%20does.))\n",
        "\n",
        "**Machine Learning (ML):** this is branch of AI which is uses statistical methods to imitate the way that humans learn (adapted from [_IBM_](https://www.ibm.com/topics/machine-learning))\n",
        "\n",
        "**Natural Language Processing (NLP):** this is branch of AI which focuses on training computers to interpret human text and spoken words (adapted from [_IBM_](https://www.ibm.com/topics/natural-language-processing#:~:text=the%20next%20step-,What%20is%20natural%20language%20processing%3F,same%20way%20human%20beings%20can.))\n",
        "\n",
        "**Word Embeddings (WE):** this is an NLP process through which human words are converted into numerical representations (usually vectors) in order for computers to be able to understand them (adapted from [_Turing_](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp))\n",
        "\n",
        "**word2vec:** this is an NLP technique that is commonly used to generate word embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR1kSHdxh6x5"
      },
      "source": [
        "## What are Word Embeddings?\n",
        "\n",
        "Building off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\n",
        "\n",
        "For example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word \"candidate\" shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, \"candidate\" in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n",
        "\n",
        "![Comparing word frequency count and word embedding methods](https://github.com/ubcecon/ai-workshop/blob/main/media/word_frequency_vs_word_embeddings.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u65pGPqh6x6"
      },
      "source": [
        "For example, in the visualization above, a word frequency count returns the number of times the word \"candidate\" or \"candidates\" is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to \"candidate\" as neighbours, while other words and phrases such as \"experimental study\" (which refers to the research paper in question, and not to candidates specifically) are further away.\n",
        "\n",
        "Here is another example of how different, but related words might be represented in a word embedding:\n",
        "<img src = \"media/w2v-Analogies.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBQpSuC2h6x6"
      },
      "source": [
        "## Making a Word Embedding\n",
        "So, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n",
        "\n",
        "![Creating a word embedding vector](https://github.com/ubcecon/ai-workshop/blob/main/media/creating_a_word_embedding.png?raw=1)\n",
        "\n",
        "The magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we'll examine a bit later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asbvtI9vh6x6"
      },
      "source": [
        "## Using word2vec\n",
        "\n",
        "Word2vec is one NLP technique that is commonly used to generate word embeddings. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool.\n",
        "\n",
        "The series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\n",
        "\n",
        "Without going too deep into the algorithm, suffice it to say that it involves a two-step process:\n",
        "\n",
        "1. First, the input word gets compressed into a dense vector, as seen in the simplified diagram, \"Creating a Word Embedding,\" above.\n",
        "2. Second, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n",
        "\n",
        "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
        "\n",
        "The two main model architectures of word2vec are **Continuous Bag of Words (CBOW)** and **Skip-Gram**, which can be distinguished partly by their input and output during training.\n",
        "\n",
        "**CBOW** takes the context words (for example, \"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/CBOW.gif?raw=1\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**Skip-Gram** does the opposite, taking a word of interest as its input (for example, \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\").\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/SG.gif?raw=1\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
        "\n",
        "Since the word embedding is a vector, we are able perform tests like cosine similarity (which we'll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4e0gm8th6x7"
      },
      "source": [
        "## Bias and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orfOT7-wh6x7"
      },
      "source": [
        "You might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n",
        "\n",
        "#### It is simply impossible to have a bias-free language model (LM).\n",
        "\n",
        "In LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” ([Arseniev-Koehler & Foster, 2020, p. 1](https://osf.io/preprints/socarxiv/b8kud/)).\n",
        "\n",
        "Just like any other computational model, it is important to critically engage with the source and context of the training data. One way that [Schiffers, Kern and Hienert](https://arxiv.org/abs/2302.06174v1) suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for \"specialized vocabulary and semantic relationships\" that can help make applications of WE more effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzNdnZr4h6x7"
      },
      "source": [
        "## Preparing for our Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abvl8sUkh6x8"
      },
      "source": [
        "#### Word2vec Features\n",
        "\n",
        "**Here are a few features of the word2vec tool that we can use to customize our analysis:**\n",
        "\n",
        "* `size`: Number of dimensions for word embedding model</li>\n",
        "* `window`: Number of context words to observe in each direction</li>\n",
        "* `min_count`: Minimum frequency for words included in model</li>\n",
        "* `sg` (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
        "* `alpha`: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
        "* `iterations`: Number of passes through dataset</li>\n",
        "* `batch size`: Number of words to sample from data during each pass</li>\n",
        "\n",
        "Note: the script uses default value for each argument.\n",
        "\n",
        "**Some limitations of the word2vec Model**\n",
        "\n",
        "* Within word2vec, common articles or conjunctions, called **stop words** such as \"the\" and \"and,\" may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\n",
        "* Word2vec isn't always the best at handling out-of-vocabulary words well (Chandran, 2021).\n",
        "\n",
        "Let's begin our analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwcWeepZh6x8"
      },
      "source": [
        "## Exercise #1: Eggs, Sausages and Bacon\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/eggs_bacon_sausages.png?raw=1\" width=\"750\"/>\n",
        "<div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykP4Ekp8h6x8"
      },
      "source": [
        "To begin, we are going to install and load a few packages that are necessary for our analysis. Run the code cells below if these packages are not already installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "81rh17nSh6x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a808a3-7161-4d73-ccf0-c27d1239a21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘Rcpp’, ‘slam’, ‘BH’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘iterators’, ‘tau’, ‘foreach’, ‘shape’, ‘RcppEigen’, ‘lars’, ‘randomForest’, ‘qdapDictionaries’, ‘ngramrr’, ‘moments’, ‘stringdist’, ‘glmnet’, ‘spikeslab’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘RcppTOML’, ‘here’, ‘png’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# uncomment these by deleting the \"#\" to install them\n",
        "\n",
        "install.packages(\"tidyverse\")\n",
        "install.packages(\"repr\")\n",
        "install.packages(\"proxy\")\n",
        "install.packages(\"scales\")\n",
        "install.packages(\"tm\")\n",
        "install.packages(\"MASS\")\n",
        "install.packages(\"SentimentAnalysis\")\n",
        "install.packages(\"reticulate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "sBPyOVNCh6x-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f85db7-4dce-4574-ffcd-0687e6e821d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
            "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
            "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
            "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.4     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
            "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
            "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
            "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
            "\n",
            "Attaching package: ‘proxy’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    as.dist, dist\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:base’:\n",
            "\n",
            "    as.matrix\n",
            "\n",
            "\n",
            "Loading required package: NLP\n",
            "\n",
            "\n",
            "Attaching package: ‘NLP’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:ggplot2’:\n",
            "\n",
            "    annotate\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘scales’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:purrr’:\n",
            "\n",
            "    discard\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:readr’:\n",
            "\n",
            "    col_factor\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘MASS’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:dplyr’:\n",
            "\n",
            "    select\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the required libraries\n",
        "library(tidyverse)\n",
        "library(repr)\n",
        "library(proxy)\n",
        "library(tm)\n",
        "library(scales)\n",
        "library(MASS)\n",
        "\n",
        "system(\"wget https://github.com/ubcecon/ai-workshop/archive/refs/tags/0.5.zip\")\n",
        "system(\"unzip 0.5.zip\")\n",
        "system(\"mkdir media && mkdir resources && mkdir txtlab_Novel450_English\")\n",
        "system(\"mv ai-workshop-0.5/media . && mv ai-workshop-0.5/resources . && mv ai-workshop-0.5/txtlab_Novel450_English .\")\n",
        "\n",
        "# Set up figures to save properly\n",
        "options(jupyter.plot_mimetypes = \"image/png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FETtxDEzh6x_"
      },
      "outputs": [],
      "source": [
        "# Time: 30s\n",
        "library(reticulate)\n",
        "gensim <- import(\"gensim\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAhOw7Tjh6x_"
      },
      "source": [
        "#### Create a Document-Term Matrix (DTM) with a Few Pseudo-Texts\n",
        "To start off, we're going to create a mini dataframe based on the use of the words \"eggs,\" \"sausages\" and \"bacon\" found in three different novels: A, B and C.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "0lbtLzqsh6x_",
        "outputId": "29c2f218-ac81-467a-b48b-7e506886b8b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        eggs sausage bacon\n",
            "Novel A   50      60    60\n",
            "Novel B   90      10    10\n",
            "Novel C   20      70    70\n"
          ]
        }
      ],
      "source": [
        "# Construct dataframe\n",
        "columns <- c('eggs', 'sausage', 'bacon')\n",
        "indices <- c('Novel A', 'Novel B', 'Novel C')\n",
        "dtm <- data.frame(eggs = c(50, 90, 20),\n",
        "                  sausage = c(60, 10, 70),\n",
        "                  bacon = c(60, 10, 70),\n",
        "                  row.names = indices)\n",
        "\n",
        "# Show dataframe\n",
        "print(dtm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JROY0EJh6yA"
      },
      "source": [
        "#### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "MrL5treuh6yA",
        "outputId": "b925c0e6-309b-439a-bf19-6615ddc1efcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC31BMVEUAAAABAQECAgIDAwME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\nRCkEJKIUAhJRCv1/DPtXpZJYT3kAAAAASUVORK5CYII="
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        }
      ],
      "source": [
        "# Then, we'll create the scatter plot of our data using ggplot2\n",
        "ggplot(dtm, aes(x = eggs, y = sausage)) +\n",
        "  geom_point() +\n",
        "  geom_text(aes(label = rownames(dtm)), nudge_x = 2, nudge_y = 2, size = 3) +\n",
        "  xlim(0, 100) +\n",
        "  ylim(0, 100) +\n",
        "  labs(x = \"eggs\", y = \"sausage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPNG2zSZh6yA"
      },
      "source": [
        "### Vectors\n",
        "\n",
        "At a glance, a couple of points are lying closer to one another. We used the word frequencies of just two words in order to plot our texts in a two-dimensional plane. The term frequency \"summaries\" of <i>Novel A</i> & <i>Novel C</i> are pretty similar to one another: they both share a major concern with \"sausage\", whereas <i>Novel B</i> seems to focus primarily on \"eggs.\"\n",
        "\n",
        "This raises a question: how can we operationalize our intuition that spatial distance expresses topical similarity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHv4yMrAh6yA"
      },
      "source": [
        "## Cosine Similarity\n",
        "The most common measurement of distance between points is their [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity). Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a _vector_.\n",
        "\n",
        "Mathematically, this can be represented as:\n",
        "<div>\n",
        "<img src='https://github.com/ubcecon/ai-workshop/blob/main/media/Dot-Product.png?raw=1' >\n",
        "\n",
        "Using our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/annotated_scatterplot.png?raw=1\" width=\"400\"/>\n",
        "<div>\n",
        "\n",
        "Because this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don't have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyCikHkzh6yA"
      },
      "source": [
        "### Calculating Cosine Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "S4K2x_7bh6yA",
        "outputId": "c85fe4f4-5046-49db-d2d2-8a60a7208398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          [,1]      [,2]     [,3]\n",
            "[1,] 1.0000000 0.6352577 0.945022\n",
            "[2,] 0.6352577 1.0000000 0.347785\n",
            "[3,] 0.9450220 0.3477850 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Assuming dtm_df is a data frame containing the document-term matrix\n",
        "dtm_matrix <- as.matrix(dtm)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim <- proxy::dist(dtm_matrix, method = \"cosine\")\n",
        "\n",
        "\n",
        "# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n",
        "# The formula for Cosine Distance is = 1 - Cosine Similarity\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "# So we will subtract the similarities from 1\n",
        "n <- nrow(dtm_matrix)\n",
        "cos_sim_array <- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n",
        "\n",
        "# Print the result\n",
        "print(cos_sim_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "wCreGyS3h6yA",
        "outputId": "6d1f1add-7e8b-4112-c404-91a69d3d44cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 3 × 3</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Novel A</th><th scope=col>Novel B</th><th scope=col>Novel C</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>Novel A</th><td>1.00</td><td>0.64</td><td>0.95</td></tr>\n",
              "\t<tr><th scope=row>Novel B</th><td>0.64</td><td>1.00</td><td>0.35</td></tr>\n",
              "\t<tr><th scope=row>Novel C</th><td>0.95</td><td>0.35</td><td>1.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 3 × 3\n\n| <!--/--> | Novel A &lt;dbl&gt; | Novel B &lt;dbl&gt; | Novel C &lt;dbl&gt; |\n|---|---|---|---|\n| Novel A | 1.00 | 0.64 | 0.95 |\n| Novel B | 0.64 | 1.00 | 0.35 |\n| Novel C | 0.95 | 0.35 | 1.00 |\n\n",
            "text/latex": "A data.frame: 3 × 3\n\\begin{tabular}{r|lll}\n  & Novel A & Novel B & Novel C\\\\\n  & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tNovel A & 1.00 & 0.64 & 0.95\\\\\n\tNovel B & 0.64 & 1.00 & 0.35\\\\\n\tNovel C & 0.95 & 0.35 & 1.00\\\\\n\\end{tabular}\n",
            "text/plain": [
              "        Novel A Novel B Novel C\n",
              "Novel A 1.00    0.64    0.95   \n",
              "Novel B 0.64    1.00    0.35   \n",
              "Novel C 0.95    0.35    1.00   "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Make it a little easier to read by rounding the values\n",
        "cos_sim_rounded <- round(cos_sim_array, 2)\n",
        "\n",
        "# Label the dataframe rows and columns with eggs, sausage and bacon\n",
        "cos_df <- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\n",
        "colnames(cos_df) <- indices\n",
        "\n",
        "# Print the data frame\n",
        "head(cos_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45hJuIBYh6yA"
      },
      "source": [
        "## Exercise #2: Working with 18th Century Literature\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ubcecon/ai-workshop/blob/main/media/18th_cent_literature.png?raw=1\" width=\"750\"/>\n",
        "<div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVsK-lbwh6yA"
      },
      "source": [
        "<font color=\"blue\" size=12>Workshop Run Here at Start</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "cPabE3aUh6yB",
        "outputId": "c828072f-90ac-4d4f-86ad-bfa5deaabe9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in readLines(filepath, encoding = \"UTF-8\"):\n",
            "“incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'”\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 7182</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>—can</th><th scope=col>—ever</th><th scope=col>—every</th><th scope=col>—except</th><th scope=col>—ill</th><th scope=col>—just</th><th scope=col>—let</th><th scope=col>—like</th><th scope=col>—may</th><th scope=col>—nay</th><th scope=col>⋯</th><th scope=col>youll</th><th scope=col>young</th><th scope=col>younger</th><th scope=col>youngest</th><th scope=col>youre</th><th scope=col>youth</th><th scope=col>youthful</th><th scope=col>youths</th><th scope=col>youve</th><th scope=col>zeal</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>Hawthorne: Scarlet Letter</th><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td><td>⋯</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td></tr>\n",
              "\t<tr><th scope=row>Hawthorne: Seven Gables</th><td>FALSE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>⋯</td><td> TRUE</td><td>TRUE</td><td>TRUE</td><td> TRUE</td><td> TRUE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td></tr>\n",
              "\t<tr><th scope=row>Fitzgerald: This Side of Paradise</th><td>FALSE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td> TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td> TRUE</td><td>TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td></tr>\n",
              "\t<tr><th scope=row>Fitzgerald: Beautiful and the Damned</th><td> TRUE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td> TRUE</td><td>TRUE</td><td>TRUE</td><td> TRUE</td><td> TRUE</td><td>TRUE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td></tr>\n",
              "\t<tr><th scope=row>Austen: Sense and Sensibility</th><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td></tr>\n",
              "\t<tr><th scope=row>Austen: Pride and Prejudice</th><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>⋯</td><td> TRUE</td><td>TRUE</td><td>TRUE</td><td> TRUE</td><td>FALSE</td><td>TRUE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 7182\n\n| <!--/--> | —can &lt;lgl&gt; | —ever &lt;lgl&gt; | —every &lt;lgl&gt; | —except &lt;lgl&gt; | —ill &lt;lgl&gt; | —just &lt;lgl&gt; | —let &lt;lgl&gt; | —like &lt;lgl&gt; | —may &lt;lgl&gt; | —nay &lt;lgl&gt; | ⋯ ⋯ | youll &lt;lgl&gt; | young &lt;lgl&gt; | younger &lt;lgl&gt; | youngest &lt;lgl&gt; | youre &lt;lgl&gt; | youth &lt;lgl&gt; | youthful &lt;lgl&gt; | youths &lt;lgl&gt; | youve &lt;lgl&gt; | zeal &lt;lgl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Hawthorne: Scarlet Letter |  TRUE |  TRUE | FALSE | FALSE |  TRUE | FALSE |  TRUE | FALSE |  TRUE |  TRUE | ⋯ | FALSE | TRUE | TRUE |  TRUE | FALSE | TRUE |  TRUE | FALSE | FALSE |  TRUE |\n| Hawthorne: Seven Gables | FALSE |  TRUE | FALSE |  TRUE | FALSE | FALSE | FALSE |  TRUE |  TRUE |  TRUE | ⋯ |  TRUE | TRUE | TRUE |  TRUE |  TRUE | TRUE |  TRUE | FALSE |  TRUE |  TRUE |\n| Fitzgerald: This Side of Paradise | FALSE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE | FALSE | FALSE | ⋯ |  TRUE | TRUE | TRUE | FALSE |  TRUE | TRUE |  TRUE |  TRUE |  TRUE | FALSE |\n| Fitzgerald: Beautiful and the Damned |  TRUE | FALSE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE | FALSE | FALSE | ⋯ |  TRUE | TRUE | TRUE |  TRUE |  TRUE | TRUE | FALSE |  TRUE |  TRUE | FALSE |\n| Austen: Sense and Sensibility | FALSE | FALSE |  TRUE | FALSE | FALSE | FALSE |  TRUE | FALSE |  TRUE | FALSE | ⋯ | FALSE | TRUE | TRUE |  TRUE | FALSE | TRUE |  TRUE | FALSE | FALSE |  TRUE |\n| Austen: Pride and Prejudice |  TRUE | FALSE | FALSE | FALSE | FALSE |  TRUE | FALSE | FALSE | FALSE |  TRUE | ⋯ |  TRUE | TRUE | TRUE |  TRUE | FALSE | TRUE | FALSE |  TRUE | FALSE | FALSE |\n\n",
            "text/latex": "A data.frame: 6 × 7182\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & —can & —ever & —every & —except & —ill & —just & —let & —like & —may & —nay & ⋯ & youll & young & younger & youngest & youre & youth & youthful & youths & youve & zeal\\\\\n  & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & ⋯ & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl> & <lgl>\\\\\n\\hline\n\tHawthorne: Scarlet Letter &  TRUE &  TRUE & FALSE & FALSE &  TRUE & FALSE &  TRUE & FALSE &  TRUE &  TRUE & ⋯ & FALSE & TRUE & TRUE &  TRUE & FALSE & TRUE &  TRUE & FALSE & FALSE &  TRUE\\\\\n\tHawthorne: Seven Gables & FALSE &  TRUE & FALSE &  TRUE & FALSE & FALSE & FALSE &  TRUE &  TRUE &  TRUE & ⋯ &  TRUE & TRUE & TRUE &  TRUE &  TRUE & TRUE &  TRUE & FALSE &  TRUE &  TRUE\\\\\n\tFitzgerald: This Side of Paradise & FALSE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE & FALSE & FALSE & ⋯ &  TRUE & TRUE & TRUE & FALSE &  TRUE & TRUE &  TRUE &  TRUE &  TRUE & FALSE\\\\\n\tFitzgerald: Beautiful and the Damned &  TRUE & FALSE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE & FALSE & FALSE & ⋯ &  TRUE & TRUE & TRUE &  TRUE &  TRUE & TRUE & FALSE &  TRUE &  TRUE & FALSE\\\\\n\tAusten: Sense and Sensibility & FALSE & FALSE &  TRUE & FALSE & FALSE & FALSE &  TRUE & FALSE &  TRUE & FALSE & ⋯ & FALSE & TRUE & TRUE &  TRUE & FALSE & TRUE &  TRUE & FALSE & FALSE &  TRUE\\\\\n\tAusten: Pride and Prejudice &  TRUE & FALSE & FALSE & FALSE & FALSE &  TRUE & FALSE & FALSE & FALSE &  TRUE & ⋯ &  TRUE & TRUE & TRUE &  TRUE & FALSE & TRUE & FALSE &  TRUE & FALSE & FALSE\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                                     —can  —ever —every —except —ill  —just\n",
              "Hawthorne: Scarlet Letter             TRUE  TRUE FALSE  FALSE    TRUE FALSE\n",
              "Hawthorne: Seven Gables              FALSE  TRUE FALSE   TRUE   FALSE FALSE\n",
              "Fitzgerald: This Side of Paradise    FALSE  TRUE  TRUE   TRUE    TRUE  TRUE\n",
              "Fitzgerald: Beautiful and the Damned  TRUE FALSE  TRUE   TRUE    TRUE  TRUE\n",
              "Austen: Sense and Sensibility        FALSE FALSE  TRUE  FALSE   FALSE FALSE\n",
              "Austen: Pride and Prejudice           TRUE FALSE FALSE  FALSE   FALSE  TRUE\n",
              "                                     —let  —like —may  —nay  ⋯ youll young\n",
              "Hawthorne: Scarlet Letter             TRUE FALSE  TRUE  TRUE ⋯ FALSE TRUE \n",
              "Hawthorne: Seven Gables              FALSE  TRUE  TRUE  TRUE ⋯  TRUE TRUE \n",
              "Fitzgerald: This Side of Paradise     TRUE  TRUE FALSE FALSE ⋯  TRUE TRUE \n",
              "Fitzgerald: Beautiful and the Damned  TRUE  TRUE FALSE FALSE ⋯  TRUE TRUE \n",
              "Austen: Sense and Sensibility         TRUE FALSE  TRUE FALSE ⋯ FALSE TRUE \n",
              "Austen: Pride and Prejudice          FALSE FALSE FALSE  TRUE ⋯  TRUE TRUE \n",
              "                                     younger youngest youre youth youthful\n",
              "Hawthorne: Scarlet Letter            TRUE     TRUE    FALSE TRUE   TRUE   \n",
              "Hawthorne: Seven Gables              TRUE     TRUE     TRUE TRUE   TRUE   \n",
              "Fitzgerald: This Side of Paradise    TRUE    FALSE     TRUE TRUE   TRUE   \n",
              "Fitzgerald: Beautiful and the Damned TRUE     TRUE     TRUE TRUE  FALSE   \n",
              "Austen: Sense and Sensibility        TRUE     TRUE    FALSE TRUE   TRUE   \n",
              "Austen: Pride and Prejudice          TRUE     TRUE    FALSE TRUE  FALSE   \n",
              "                                     youths youve zeal \n",
              "Hawthorne: Scarlet Letter            FALSE  FALSE  TRUE\n",
              "Hawthorne: Seven Gables              FALSE   TRUE  TRUE\n",
              "Fitzgerald: This Side of Paradise     TRUE   TRUE FALSE\n",
              "Fitzgerald: Beautiful and the Damned  TRUE   TRUE FALSE\n",
              "Austen: Sense and Sensibility        FALSE  FALSE  TRUE\n",
              "Austen: Pride and Prejudice           TRUE  FALSE FALSE"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the required libraries\n",
        "library(tidyverse)\n",
        "library(repr)\n",
        "library(proxy)\n",
        "library(tm)\n",
        "library(scales)\n",
        "library(MASS)\n",
        "\n",
        "\n",
        "# Set up figures to save properly\n",
        "options(jupyter.plot_mimetypes = \"image/png\")\n",
        "\n",
        "# Time: 3 mins\n",
        "# File paths and names\n",
        "filelist <- c(\n",
        "  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n",
        ")\n",
        "\n",
        "novel_names <- c(\n",
        "  'Hawthorne: Scarlet Letter',\n",
        "  'Hawthorne: Seven Gables',\n",
        "  'Fitzgerald: This Side of Paradise',\n",
        "  'Fitzgerald: Beautiful and the Damned',\n",
        "  'Austen: Sense and Sensibility',\n",
        "  'Austen: Pride and Prejudice'\n",
        ")\n",
        "\n",
        "# Function to read non-empty lines from the text file\n",
        "readNonEmptyLines <- function(filepath) {\n",
        "  lines <- readLines(filepath, encoding = \"UTF-8\")\n",
        "  non_empty_lines <- lines[trimws(lines) != \"\"]\n",
        "  return(paste(non_empty_lines, collapse = \" \"))\n",
        "}\n",
        "\n",
        "# Read non-empty texts into a corpus\n",
        "text_corpus <- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n",
        "\n",
        "# Preprocess the text data\n",
        "text_corpus <- tm_map(text_corpus, content_transformer(tolower))\n",
        "text_corpus <- tm_map(text_corpus, removePunctuation)\n",
        "text_corpus <- tm_map(text_corpus, removeNumbers)\n",
        "text_corpus <- tm_map(text_corpus, removeWords, stopwords(\"english\"))\n",
        "text_corpus <- tm_map(text_corpus, stripWhitespace)\n",
        "\n",
        "## Time: 5 mins\n",
        "# Create a custom control for DTM with binary term frequency\n",
        "custom_control <- list(\n",
        "  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n",
        "  bounds = list(global = c(3, Inf)),\n",
        "  weighting = weightTf\n",
        ")\n",
        "\n",
        "# Convert the corpus to a DTM using custom control\n",
        "dtm <- DocumentTermMatrix(text_corpus, control = custom_control)\n",
        "\n",
        "# Convert DTM to a binary data frame (0 or 1)\n",
        "dtm_df_novel <- as.data.frame(as.matrix(dtm > 0))\n",
        "colnames(dtm_df_novel) <- colnames(dtm)\n",
        "\n",
        "# Set row names to novel names\n",
        "rownames(dtm_df_novel) <- novel_names\n",
        "\n",
        "# Print the resulting data frame\n",
        "tail(dtm_df_novel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8_rwT3lzh6yB",
        "outputId": "38d84953-4d22-48c6-ab8d-482270d33561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     [,1] [,2] [,3] [,4] [,5] [,6]\n",
            "[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n",
            "[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n",
            "[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n",
            "[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n",
            "[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n",
            "[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n"
          ]
        }
      ],
      "source": [
        "# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\n",
        "cos_sim_novel <- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "n <- nrow(dtm_df_novel)\n",
        "cos_sim_array <- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n",
        "\n",
        "# Round the cosine similarity matrix to two decimal places\n",
        "cos_sim_novel_rounded <- round(cos_sim_array, 2)\n",
        "\n",
        "# Print the rounded cosine similarity matrix\n",
        "print(cos_sim_novel_rounded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WhXrzbhRh6yB",
        "outputId": "7c3e78db-ba15-4088-d3d9-f150a575ec66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Hawthorne: Scarlet Letter</th><th scope=col>Hawthorne: Seven Gables</th><th scope=col>Fitzgerald: This Side of Paradise</th><th scope=col>Fitzgerald: Beautiful and the Damned</th><th scope=col>Austen: Sense and Sensibility</th><th scope=col>Austen: Pride and Prejudice</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>Hawthorne: Scarlet Letter</th><td>1.00</td><td>0.80</td><td>0.69</td><td>0.75</td><td>0.68</td><td>0.67</td></tr>\n",
              "\t<tr><th scope=row>Hawthorne: Seven Gables</th><td>0.80</td><td>1.00</td><td>0.74</td><td>0.80</td><td>0.71</td><td>0.70</td></tr>\n",
              "\t<tr><th scope=row>Fitzgerald: This Side of Paradise</th><td>0.69</td><td>0.74</td><td>1.00</td><td>0.79</td><td>0.63</td><td>0.62</td></tr>\n",
              "\t<tr><th scope=row>Fitzgerald: Beautiful and the Damned</th><td>0.75</td><td>0.80</td><td>0.79</td><td>1.00</td><td>0.70</td><td>0.69</td></tr>\n",
              "\t<tr><th scope=row>Austen: Sense and Sensibility</th><td>0.68</td><td>0.71</td><td>0.63</td><td>0.70</td><td>1.00</td><td>0.81</td></tr>\n",
              "\t<tr><th scope=row>Austen: Pride and Prejudice</th><td>0.67</td><td>0.70</td><td>0.62</td><td>0.69</td><td>0.81</td><td>1.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A data.frame: 6 × 6\n",
              "\\begin{tabular}{r|llllll}\n",
              "  & Hawthorne: Scarlet Letter & Hawthorne: Seven Gables & Fitzgerald: This Side of Paradise & Fitzgerald: Beautiful and the Damned & Austen: Sense and Sensibility & Austen: Pride and Prejudice\\\\\n",
              "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
              "\\hline\n",
              "\tHawthorne: Scarlet Letter & 1.00 & 0.80 & 0.69 & 0.75 & 0.68 & 0.67\\\\\n",
              "\tHawthorne: Seven Gables & 0.80 & 1.00 & 0.74 & 0.80 & 0.71 & 0.70\\\\\n",
              "\tFitzgerald: This Side of Paradise & 0.69 & 0.74 & 1.00 & 0.79 & 0.63 & 0.62\\\\\n",
              "\tFitzgerald: Beautiful and the Damned & 0.75 & 0.80 & 0.79 & 1.00 & 0.70 & 0.69\\\\\n",
              "\tAusten: Sense and Sensibility & 0.68 & 0.71 & 0.63 & 0.70 & 1.00 & 0.81\\\\\n",
              "\tAusten: Pride and Prejudice & 0.67 & 0.70 & 0.62 & 0.69 & 0.81 & 1.00\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A data.frame: 6 × 6\n",
              "\n",
              "| <!--/--> | Hawthorne: Scarlet Letter &lt;dbl&gt; | Hawthorne: Seven Gables &lt;dbl&gt; | Fitzgerald: This Side of Paradise &lt;dbl&gt; | Fitzgerald: Beautiful and the Damned &lt;dbl&gt; | Austen: Sense and Sensibility &lt;dbl&gt; | Austen: Pride and Prejudice &lt;dbl&gt; |\n",
              "|---|---|---|---|---|---|---|\n",
              "| Hawthorne: Scarlet Letter | 1.00 | 0.80 | 0.69 | 0.75 | 0.68 | 0.67 |\n",
              "| Hawthorne: Seven Gables | 0.80 | 1.00 | 0.74 | 0.80 | 0.71 | 0.70 |\n",
              "| Fitzgerald: This Side of Paradise | 0.69 | 0.74 | 1.00 | 0.79 | 0.63 | 0.62 |\n",
              "| Fitzgerald: Beautiful and the Damned | 0.75 | 0.80 | 0.79 | 1.00 | 0.70 | 0.69 |\n",
              "| Austen: Sense and Sensibility | 0.68 | 0.71 | 0.63 | 0.70 | 1.00 | 0.81 |\n",
              "| Austen: Pride and Prejudice | 0.67 | 0.70 | 0.62 | 0.69 | 0.81 | 1.00 |\n",
              "\n"
            ],
            "text/plain": [
              "                                     Hawthorne: Scarlet Letter\n",
              "Hawthorne: Scarlet Letter            1.00                     \n",
              "Hawthorne: Seven Gables              0.80                     \n",
              "Fitzgerald: This Side of Paradise    0.69                     \n",
              "Fitzgerald: Beautiful and the Damned 0.75                     \n",
              "Austen: Sense and Sensibility        0.68                     \n",
              "Austen: Pride and Prejudice          0.67                     \n",
              "                                     Hawthorne: Seven Gables\n",
              "Hawthorne: Scarlet Letter            0.80                   \n",
              "Hawthorne: Seven Gables              1.00                   \n",
              "Fitzgerald: This Side of Paradise    0.74                   \n",
              "Fitzgerald: Beautiful and the Damned 0.80                   \n",
              "Austen: Sense and Sensibility        0.71                   \n",
              "Austen: Pride and Prejudice          0.70                   \n",
              "                                     Fitzgerald: This Side of Paradise\n",
              "Hawthorne: Scarlet Letter            0.69                             \n",
              "Hawthorne: Seven Gables              0.74                             \n",
              "Fitzgerald: This Side of Paradise    1.00                             \n",
              "Fitzgerald: Beautiful and the Damned 0.79                             \n",
              "Austen: Sense and Sensibility        0.63                             \n",
              "Austen: Pride and Prejudice          0.62                             \n",
              "                                     Fitzgerald: Beautiful and the Damned\n",
              "Hawthorne: Scarlet Letter            0.75                                \n",
              "Hawthorne: Seven Gables              0.80                                \n",
              "Fitzgerald: This Side of Paradise    0.79                                \n",
              "Fitzgerald: Beautiful and the Damned 1.00                                \n",
              "Austen: Sense and Sensibility        0.70                                \n",
              "Austen: Pride and Prejudice          0.69                                \n",
              "                                     Austen: Sense and Sensibility\n",
              "Hawthorne: Scarlet Letter            0.68                         \n",
              "Hawthorne: Seven Gables              0.71                         \n",
              "Fitzgerald: This Side of Paradise    0.63                         \n",
              "Fitzgerald: Beautiful and the Damned 0.70                         \n",
              "Austen: Sense and Sensibility        1.00                         \n",
              "Austen: Pride and Prejudice          0.81                         \n",
              "                                     Austen: Pride and Prejudice\n",
              "Hawthorne: Scarlet Letter            0.67                       \n",
              "Hawthorne: Seven Gables              0.70                       \n",
              "Fitzgerald: This Side of Paradise    0.62                       \n",
              "Fitzgerald: Beautiful and the Damned 0.69                       \n",
              "Austen: Sense and Sensibility        0.81                       \n",
              "Austen: Pride and Prejudice          1.00                       "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Again, we'll make this a bit more readable\n",
        "cos_df <- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n",
        "\n",
        "# Set column names to novel names\n",
        "colnames(cos_df) <- novel_names\n",
        "\n",
        "# Print the DataFrame\n",
        "head(cos_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8tqqS7vJh6yB",
        "outputId": "1ae0573d-3904-4562-f5ad-2f7c7bb57d04"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAh1BMVEUAAAAyMjJERERHR0dN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"
          },
          "metadata": {
            "image/png": {
              "height": 420,
              "width": 420
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Transform cosine similarity to cosine distance\n",
        "cos_dist <- 1 - cos_sim_novel_rounded\n",
        "\n",
        "# Perform MDS\n",
        "mds <- cmdscale(cos_dist, k = 2)\n",
        "\n",
        "# Extract x and y coordinates from MDS output\n",
        "xs <- mds[, 1]\n",
        "ys <- mds[, 2]\n",
        "\n",
        "# Create a data frame with x, y coordinates, and novel names\n",
        "mds_df <- data.frame(x = xs, y = ys, novel_names = novel_names)\n",
        "\n",
        "ggplot(mds_df, aes(x, y, label = novel_names)) +\n",
        "  geom_point(size = 4) +\n",
        "  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n",
        "  labs(title = \"MDS Visualization of Novel Differences\") +\n",
        "  theme_minimal() +\n",
        "  theme(\n",
        "    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n",
        "    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n",
        "    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n",
        "    plot.caption = element_blank(),  # Remove the default caption\n",
        "    axis.text = element_text(size = 12),  # Adjust the size of axis text\n",
        "    legend.text = element_text(size = 12),  # Adjust the size of legend text\n",
        "    legend.title = element_text(size = 14)  # Adjust the size of legend title\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHMPA7XIh6yB"
      },
      "source": [
        "The above method has a broad range of applications, such as unsupervised clustering. Common techniques include <a href = \"https://en.wikipedia.org/wiki/K-means_clustering\">K-Means Clustering</a> and <a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">Hierarchical Dendrograms</a>. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\n",
        "\n",
        "Here's an example of a dendrogram based on these six novels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UABV7b97h6yB",
        "outputId": "62582bda-0ac4-40fd-d6f6-907c5b93037b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRU5ErkJggg=="
          },
          "metadata": {
            "image/png": {
              "height": 420,
              "width": 420
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "hclust_result <- hclust(as.dist(cos_dist), method = \"ward.D\")\n",
        "\n",
        "# Plot the dendrogram\n",
        "plot(hclust_result, hang = -1, labels = novel_names)\n",
        "\n",
        "# Optional: Adjust the layout to avoid cutoff labels\n",
        "par(mar = c(5, 4, 2, 10))  # Adjust margins\n",
        "\n",
        "# Display the dendrogram plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rLUgU8Sh6yC"
      },
      "source": [
        "#### Vector Semantics\n",
        "\n",
        "We can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PyCW-Go3h6yC",
        "outputId": "18ae66ad-fce7-4cfb-be56-258ad7c6d4b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 6 × 6 of type lgl</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Hawthorne: Scarlet Letter</th><th scope=col>Hawthorne: Seven Gables</th><th scope=col>Fitzgerald: This Side of Paradise</th><th scope=col>Fitzgerald: Beautiful and the Damned</th><th scope=col>Austen: Sense and Sensibility</th><th scope=col>Austen: Pride and Prejudice</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>youre</th><td>FALSE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td></tr>\n",
              "\t<tr><th scope=row>youth</th><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td></tr>\n",
              "\t<tr><th scope=row>youthful</th><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td></tr>\n",
              "\t<tr><th scope=row>youths</th><td>FALSE</td><td>FALSE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td> TRUE</td></tr>\n",
              "\t<tr><th scope=row>youve</th><td>FALSE</td><td> TRUE</td><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td></tr>\n",
              "\t<tr><th scope=row>zeal</th><td> TRUE</td><td> TRUE</td><td>FALSE</td><td>FALSE</td><td> TRUE</td><td>FALSE</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A matrix: 6 × 6 of type lgl\n",
              "\\begin{tabular}{r|llllll}\n",
              "  & Hawthorne: Scarlet Letter & Hawthorne: Seven Gables & Fitzgerald: This Side of Paradise & Fitzgerald: Beautiful and the Damned & Austen: Sense and Sensibility & Austen: Pride and Prejudice\\\\\n",
              "\\hline\n",
              "\tyoure & FALSE &  TRUE &  TRUE &  TRUE & FALSE & FALSE\\\\\n",
              "\tyouth &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE &  TRUE\\\\\n",
              "\tyouthful &  TRUE &  TRUE &  TRUE & FALSE &  TRUE & FALSE\\\\\n",
              "\tyouths & FALSE & FALSE &  TRUE &  TRUE & FALSE &  TRUE\\\\\n",
              "\tyouve & FALSE &  TRUE &  TRUE &  TRUE & FALSE & FALSE\\\\\n",
              "\tzeal &  TRUE &  TRUE & FALSE & FALSE &  TRUE & FALSE\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A matrix: 6 × 6 of type lgl\n",
              "\n",
              "| <!--/--> | Hawthorne: Scarlet Letter | Hawthorne: Seven Gables | Fitzgerald: This Side of Paradise | Fitzgerald: Beautiful and the Damned | Austen: Sense and Sensibility | Austen: Pride and Prejudice |\n",
              "|---|---|---|---|---|---|---|\n",
              "| youre | FALSE |  TRUE |  TRUE |  TRUE | FALSE | FALSE |\n",
              "| youth |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE |  TRUE |\n",
              "| youthful |  TRUE |  TRUE |  TRUE | FALSE |  TRUE | FALSE |\n",
              "| youths | FALSE | FALSE |  TRUE |  TRUE | FALSE |  TRUE |\n",
              "| youve | FALSE |  TRUE |  TRUE |  TRUE | FALSE | FALSE |\n",
              "| zeal |  TRUE |  TRUE | FALSE | FALSE |  TRUE | FALSE |\n",
              "\n"
            ],
            "text/plain": [
              "         Hawthorne: Scarlet Letter Hawthorne: Seven Gables\n",
              "youre    FALSE                      TRUE                  \n",
              "youth     TRUE                      TRUE                  \n",
              "youthful  TRUE                      TRUE                  \n",
              "youths   FALSE                     FALSE                  \n",
              "youve    FALSE                      TRUE                  \n",
              "zeal      TRUE                      TRUE                  \n",
              "         Fitzgerald: This Side of Paradise Fitzgerald: Beautiful and the Damned\n",
              "youre     TRUE                              TRUE                               \n",
              "youth     TRUE                              TRUE                               \n",
              "youthful  TRUE                             FALSE                               \n",
              "youths    TRUE                              TRUE                               \n",
              "youve     TRUE                              TRUE                               \n",
              "zeal     FALSE                             FALSE                               \n",
              "         Austen: Sense and Sensibility Austen: Pride and Prejudice\n",
              "youre    FALSE                         FALSE                      \n",
              "youth     TRUE                          TRUE                      \n",
              "youthful  TRUE                         FALSE                      \n",
              "youths   FALSE                          TRUE                      \n",
              "youve    FALSE                         FALSE                      \n",
              "zeal      TRUE                         FALSE                      "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Transpose the DTM data frame\n",
        "transposed_dtm <- t(dtm_df_novel)\n",
        "\n",
        "# Display the first few rows of the transposed DTM\n",
        "tail(transposed_dtm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY_UdlGEh6yC"
      },
      "source": [
        "Because the number of words is so large, for memory reasons we're going to work with just the last few, pictured above.  \n",
        "\n",
        "* If you are running this locally, you may want to try this with more words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7jgN-QNBh6yC",
        "outputId": "30ffe50d-3ee2-40fb-c832-9321e56b3e76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 6 × 6 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1.0000000</td><td>0.7071068</td><td>0.5773503</td><td>0.6666667</td><td>1.0000000</td><td>0.3333333</td></tr>\n",
              "\t<tr><td>0.7071068</td><td>1.0000000</td><td>0.8164966</td><td>0.7071068</td><td>0.7071068</td><td>0.7071068</td></tr>\n",
              "\t<tr><td>0.5773503</td><td>0.8164966</td><td>1.0000000</td><td>0.2886751</td><td>0.5773503</td><td>0.8660254</td></tr>\n",
              "\t<tr><td>0.6666667</td><td>0.7071068</td><td>0.2886751</td><td>1.0000000</td><td>0.6666667</td><td>0.0000000</td></tr>\n",
              "\t<tr><td>1.0000000</td><td>0.7071068</td><td>0.5773503</td><td>0.6666667</td><td>1.0000000</td><td>0.3333333</td></tr>\n",
              "\t<tr><td>0.3333333</td><td>0.7071068</td><td>0.8660254</td><td>0.0000000</td><td>0.3333333</td><td>1.0000000</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A matrix: 6 × 6 of type dbl\n",
              "\\begin{tabular}{llllll}\n",
              "\t 1.0000000 & 0.7071068 & 0.5773503 & 0.6666667 & 1.0000000 & 0.3333333\\\\\n",
              "\t 0.7071068 & 1.0000000 & 0.8164966 & 0.7071068 & 0.7071068 & 0.7071068\\\\\n",
              "\t 0.5773503 & 0.8164966 & 1.0000000 & 0.2886751 & 0.5773503 & 0.8660254\\\\\n",
              "\t 0.6666667 & 0.7071068 & 0.2886751 & 1.0000000 & 0.6666667 & 0.0000000\\\\\n",
              "\t 1.0000000 & 0.7071068 & 0.5773503 & 0.6666667 & 1.0000000 & 0.3333333\\\\\n",
              "\t 0.3333333 & 0.7071068 & 0.8660254 & 0.0000000 & 0.3333333 & 1.0000000\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A matrix: 6 × 6 of type dbl\n",
              "\n",
              "| 1.0000000 | 0.7071068 | 0.5773503 | 0.6666667 | 1.0000000 | 0.3333333 |\n",
              "| 0.7071068 | 1.0000000 | 0.8164966 | 0.7071068 | 0.7071068 | 0.7071068 |\n",
              "| 0.5773503 | 0.8164966 | 1.0000000 | 0.2886751 | 0.5773503 | 0.8660254 |\n",
              "| 0.6666667 | 0.7071068 | 0.2886751 | 1.0000000 | 0.6666667 | 0.0000000 |\n",
              "| 1.0000000 | 0.7071068 | 0.5773503 | 0.6666667 | 1.0000000 | 0.3333333 |\n",
              "| 0.3333333 | 0.7071068 | 0.8660254 | 0.0000000 | 0.3333333 | 1.0000000 |\n",
              "\n"
            ],
            "text/plain": [
              "     [,1]      [,2]      [,3]      [,4]      [,5]      [,6]     \n",
              "[1,] 1.0000000 0.7071068 0.5773503 0.6666667 1.0000000 0.3333333\n",
              "[2,] 0.7071068 1.0000000 0.8164966 0.7071068 0.7071068 0.7071068\n",
              "[3,] 0.5773503 0.8164966 1.0000000 0.2886751 0.5773503 0.8660254\n",
              "[4,] 0.6666667 0.7071068 0.2886751 1.0000000 0.6666667 0.0000000\n",
              "[5,] 1.0000000 0.7071068 0.5773503 0.6666667 1.0000000 0.3333333\n",
              "[6,] 0.3333333 0.7071068 0.8660254 0.0000000 0.3333333 1.0000000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Assuming dtm_df is a data frame containing the document-term matrix\n",
        "tail_transposed_dtm <- tail(transposed_dtm)\n",
        "\n",
        "dtm_matrix <- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim_words <- proxy::dist(dtm_matrix, method = \"cosine\")\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "n <- nrow(dtm_matrix)\n",
        "cos_sim_words <- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n",
        "\n",
        "# Print the result\n",
        "head(cos_sim_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "a4D0ACHXh6yC",
        "outputId": "9ea31d2a-250c-49e4-fcc6-d8260dc5f09c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>youre</th><th scope=col>youth</th><th scope=col>youthful</th><th scope=col>youths</th><th scope=col>youve</th><th scope=col>zeal</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>youre</th><td>1.00</td><td>0.71</td><td>0.58</td><td>0.67</td><td>1.00</td><td>0.33</td></tr>\n",
              "\t<tr><th scope=row>youth</th><td>0.71</td><td>1.00</td><td>0.82</td><td>0.71</td><td>0.71</td><td>0.71</td></tr>\n",
              "\t<tr><th scope=row>youthful</th><td>0.58</td><td>0.82</td><td>1.00</td><td>0.29</td><td>0.58</td><td>0.87</td></tr>\n",
              "\t<tr><th scope=row>youths</th><td>0.67</td><td>0.71</td><td>0.29</td><td>1.00</td><td>0.67</td><td>0.00</td></tr>\n",
              "\t<tr><th scope=row>youve</th><td>1.00</td><td>0.71</td><td>0.58</td><td>0.67</td><td>1.00</td><td>0.33</td></tr>\n",
              "\t<tr><th scope=row>zeal</th><td>0.33</td><td>0.71</td><td>0.87</td><td>0.00</td><td>0.33</td><td>1.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A data.frame: 6 × 6\n",
              "\\begin{tabular}{r|llllll}\n",
              "  & youre & youth & youthful & youths & youve & zeal\\\\\n",
              "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
              "\\hline\n",
              "\tyoure & 1.00 & 0.71 & 0.58 & 0.67 & 1.00 & 0.33\\\\\n",
              "\tyouth & 0.71 & 1.00 & 0.82 & 0.71 & 0.71 & 0.71\\\\\n",
              "\tyouthful & 0.58 & 0.82 & 1.00 & 0.29 & 0.58 & 0.87\\\\\n",
              "\tyouths & 0.67 & 0.71 & 0.29 & 1.00 & 0.67 & 0.00\\\\\n",
              "\tyouve & 1.00 & 0.71 & 0.58 & 0.67 & 1.00 & 0.33\\\\\n",
              "\tzeal & 0.33 & 0.71 & 0.87 & 0.00 & 0.33 & 1.00\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A data.frame: 6 × 6\n",
              "\n",
              "| <!--/--> | youre &lt;dbl&gt; | youth &lt;dbl&gt; | youthful &lt;dbl&gt; | youths &lt;dbl&gt; | youve &lt;dbl&gt; | zeal &lt;dbl&gt; |\n",
              "|---|---|---|---|---|---|---|\n",
              "| youre | 1.00 | 0.71 | 0.58 | 0.67 | 1.00 | 0.33 |\n",
              "| youth | 0.71 | 1.00 | 0.82 | 0.71 | 0.71 | 0.71 |\n",
              "| youthful | 0.58 | 0.82 | 1.00 | 0.29 | 0.58 | 0.87 |\n",
              "| youths | 0.67 | 0.71 | 0.29 | 1.00 | 0.67 | 0.00 |\n",
              "| youve | 1.00 | 0.71 | 0.58 | 0.67 | 1.00 | 0.33 |\n",
              "| zeal | 0.33 | 0.71 | 0.87 | 0.00 | 0.33 | 1.00 |\n",
              "\n"
            ],
            "text/plain": [
              "         youre youth youthful youths youve zeal\n",
              "youre    1.00  0.71  0.58     0.67   1.00  0.33\n",
              "youth    0.71  1.00  0.82     0.71   0.71  0.71\n",
              "youthful 0.58  0.82  1.00     0.29   0.58  0.87\n",
              "youths   0.67  0.71  0.29     1.00   0.67  0.00\n",
              "youve    1.00  0.71  0.58     0.67   1.00  0.33\n",
              "zeal     0.33  0.71  0.87     0.00   0.33  1.00"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# In readable format\n",
        "\n",
        "cos_sim_words <- data.frame(round(cos_sim_words, 2))\n",
        "row.names(cos_sim_words) <- row.names(tail_transposed_dtm) #remove tail_ for all\n",
        "colnames(cos_sim_words) <- row.names(tail_transposed_dtm) #remove tail_ for all\n",
        "\n",
        "head(cos_sim_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3yZ1rhh6yC"
      },
      "source": [
        "Theoretically we could visualize and cluster these as well - but it would a lot of computational power!\n",
        "\n",
        "We'll instead turn to the machine learning version: word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqKwUXqth6yF",
        "outputId": "5e851ee4-e9d5-4ef6-bebc-60fdf41df860"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>filelist</dt><dd>'1.1 Kb'</dd><dt>dtm_df_novel</dt><dd>'1.1 Mb'</dd><dt>dtm_matrix</dt><dd>'1.5 Kb'</dd><dt>tail_transposed_dtm</dt><dd>'1.5 Kb'</dd><dt>mds_df</dt><dd>'1.6 Kb'</dd><dt>cos_sim_novel</dt><dd>'1.9 Kb'</dd><dt>custom_control</dt><dd>'14.6 Kb'</dd><dt>cos_sim_words</dt><dd>'2 Kb'</dd><dt>hclust_result</dt><dd>'2.1 Kb'</dd><dt>text_corpus</dt><dd>'2.1 Mb'</dd><dt>cos_sim</dt><dd>'2.2 Kb'</dd><dt>cos_df</dt><dd>'2.3 Kb'</dd><dt>readNonEmptyLines</dt><dd>'23.9 Kb'</dd><dt>columns</dt><dd>'248 bytes'</dd><dt>indices</dt><dd>'248 bytes'</dd><dt>cos_sim_rounded</dt><dd>'344 bytes'</dd><dt>gensim</dt><dd>'392 bytes'</dd><dt>cos_dist</dt><dd>'504 bytes'</dd><dt>cos_sim_array</dt><dd>'504 bytes'</dd><dt>cos_sim_novel_rounded</dt><dd>'504 bytes'</dd><dt>mds</dt><dd>'520 bytes'</dd><dt>n</dt><dd>'56 bytes'</dd><dt>novel_names</dt><dd>'608 bytes'</dd><dt>transposed_dtm</dt><dd>'642.5 Kb'</dd><dt>dtm</dt><dd>'954.9 Kb'</dd><dt>xs</dt><dd>'96 bytes'</dd><dt>ys</dt><dd>'96 bytes'</dd></dl>\n"
            ],
            "text/latex": [
              "\\begin{description*}\n",
              "\\item[filelist] '1.1 Kb'\n",
              "\\item[dtm\\textbackslash{}\\_df\\textbackslash{}\\_novel] '1.1 Mb'\n",
              "\\item[dtm\\textbackslash{}\\_matrix] '1.5 Kb'\n",
              "\\item[tail\\textbackslash{}\\_transposed\\textbackslash{}\\_dtm] '1.5 Kb'\n",
              "\\item[mds\\textbackslash{}\\_df] '1.6 Kb'\n",
              "\\item[cos\\textbackslash{}\\_sim\\textbackslash{}\\_novel] '1.9 Kb'\n",
              "\\item[custom\\textbackslash{}\\_control] '14.6 Kb'\n",
              "\\item[cos\\textbackslash{}\\_sim\\textbackslash{}\\_words] '2 Kb'\n",
              "\\item[hclust\\textbackslash{}\\_result] '2.1 Kb'\n",
              "\\item[text\\textbackslash{}\\_corpus] '2.1 Mb'\n",
              "\\item[cos\\textbackslash{}\\_sim] '2.2 Kb'\n",
              "\\item[cos\\textbackslash{}\\_df] '2.3 Kb'\n",
              "\\item[readNonEmptyLines] '23.9 Kb'\n",
              "\\item[columns] '248 bytes'\n",
              "\\item[indices] '248 bytes'\n",
              "\\item[cos\\textbackslash{}\\_sim\\textbackslash{}\\_rounded] '344 bytes'\n",
              "\\item[gensim] '392 bytes'\n",
              "\\item[cos\\textbackslash{}\\_dist] '504 bytes'\n",
              "\\item[cos\\textbackslash{}\\_sim\\textbackslash{}\\_array] '504 bytes'\n",
              "\\item[cos\\textbackslash{}\\_sim\\textbackslash{}\\_novel\\textbackslash{}\\_rounded] '504 bytes'\n",
              "\\item[mds] '520 bytes'\n",
              "\\item[n] '56 bytes'\n",
              "\\item[novel\\textbackslash{}\\_names] '608 bytes'\n",
              "\\item[transposed\\textbackslash{}\\_dtm] '642.5 Kb'\n",
              "\\item[dtm] '954.9 Kb'\n",
              "\\item[xs] '96 bytes'\n",
              "\\item[ys] '96 bytes'\n",
              "\\end{description*}\n"
            ],
            "text/markdown": [
              "filelist\n",
              ":   '1.1 Kb'dtm_df_novel\n",
              ":   '1.1 Mb'dtm_matrix\n",
              ":   '1.5 Kb'tail_transposed_dtm\n",
              ":   '1.5 Kb'mds_df\n",
              ":   '1.6 Kb'cos_sim_novel\n",
              ":   '1.9 Kb'custom_control\n",
              ":   '14.6 Kb'cos_sim_words\n",
              ":   '2 Kb'hclust_result\n",
              ":   '2.1 Kb'text_corpus\n",
              ":   '2.1 Mb'cos_sim\n",
              ":   '2.2 Kb'cos_df\n",
              ":   '2.3 Kb'readNonEmptyLines\n",
              ":   '23.9 Kb'columns\n",
              ":   '248 bytes'indices\n",
              ":   '248 bytes'cos_sim_rounded\n",
              ":   '344 bytes'gensim\n",
              ":   '392 bytes'cos_dist\n",
              ":   '504 bytes'cos_sim_array\n",
              ":   '504 bytes'cos_sim_novel_rounded\n",
              ":   '504 bytes'mds\n",
              ":   '520 bytes'n\n",
              ":   '56 bytes'novel_names\n",
              ":   '608 bytes'transposed_dtm\n",
              ":   '642.5 Kb'dtm\n",
              ":   '954.9 Kb'xs\n",
              ":   '96 bytes'ys\n",
              ":   '96 bytes'\n",
              "\n"
            ],
            "text/plain": [
              "             filelist          dtm_df_novel            dtm_matrix \n",
              "             \"1.1 Kb\"              \"1.1 Mb\"              \"1.5 Kb\" \n",
              "  tail_transposed_dtm                mds_df         cos_sim_novel \n",
              "             \"1.5 Kb\"              \"1.6 Kb\"              \"1.9 Kb\" \n",
              "       custom_control         cos_sim_words         hclust_result \n",
              "            \"14.6 Kb\"                \"2 Kb\"              \"2.1 Kb\" \n",
              "          text_corpus               cos_sim                cos_df \n",
              "             \"2.1 Mb\"              \"2.2 Kb\"              \"2.3 Kb\" \n",
              "    readNonEmptyLines               columns               indices \n",
              "            \"23.9 Kb\"           \"248 bytes\"           \"248 bytes\" \n",
              "      cos_sim_rounded                gensim              cos_dist \n",
              "          \"344 bytes\"           \"392 bytes\"           \"504 bytes\" \n",
              "        cos_sim_array cos_sim_novel_rounded                   mds \n",
              "          \"504 bytes\"           \"504 bytes\"           \"520 bytes\" \n",
              "                    n           novel_names        transposed_dtm \n",
              "           \"56 bytes\"           \"608 bytes\"            \"642.5 Kb\" \n",
              "                  dtm                    xs                    ys \n",
              "           \"954.9 Kb\"            \"96 bytes\"            \"96 bytes\" "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#check objects in memory; delete the big ones\n",
        "\n",
        "sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n",
        "\n",
        "rm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n",
        "\n",
        "sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSj7LP2wh6yG"
      },
      "source": [
        "## Exercise #3: Using Word2vec with 150 English Novels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtq2vGhqh6yG"
      },
      "source": [
        "In this exercise, we'll use an English-language subset from a dataset about novels created by [Andrew Piper](https://www.mcgill.ca/langlitcultures/andrew-piper). Specifically we'll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
        "\n",
        "#### Metadata Columns\n",
        "<ol><li>Filename: Name of file on disk</li>\n",
        "<li>ID: Unique ID in Piper corpus</li>\n",
        "<li>Language: Language of novel</li>\n",
        "<li>Date: Initial publication date</li>\n",
        "<li>Title: Title of novel</li>\n",
        "<li>Gender: Authorial gender</li>\n",
        "<li>Person: Textual perspective</li>\n",
        "<li>Length: Number of tokens in novel</li></ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYlM_ELh6yG"
      },
      "source": [
        "#### Import Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hD1amK72h6yG"
      },
      "outputs": [],
      "source": [
        "# Import Metadata into Dataframe\n",
        "meta_df <- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "C9ZAdv31h6yG",
        "outputId": "8515dc20-765e-4554-fd2f-f523b7563200"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 9</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>filename</th><th scope=col>id</th><th scope=col>language</th><th scope=col>date</th><th scope=col>author</th><th scope=col>title</th><th scope=col>gender</th><th scope=col>person</th><th scope=col>length</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>EN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt             </td><td>151</td><td>English</td><td>1771</td><td>Mackenzie,Henry    </td><td>TheManofFeeling             </td><td>male  </td><td>first</td><td> 36458</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>EN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt</td><td>152</td><td>English</td><td>1771</td><td>Smollett,Tobias    </td><td>TheExpedictionofHenryClinker</td><td>male  </td><td>first</td><td>148261</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>EN_1778_Burney,Fanny_Evelina_Novel.txt                        </td><td>153</td><td>English</td><td>1778</td><td>Burney,Fanny       </td><td>Evelina                     </td><td>female</td><td>first</td><td>154168</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>EN_1782_Burney,Fanny_Cecilia_Novel.txt                        </td><td>154</td><td>English</td><td>1782</td><td>Burney,Fanny       </td><td>Cecilia                     </td><td>female</td><td>third</td><td>328981</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>EN_1786_Beckford,William_Vathek_Novel.txt                     </td><td>155</td><td>English</td><td>1786</td><td>Beckford,William   </td><td>Vathek                      </td><td>male  </td><td>third</td><td> 36077</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>EN_1788_Wollstonecraft,Mary_Mary_Novel.txt                    </td><td>156</td><td>English</td><td>1788</td><td>Wollstonecraft,Mary</td><td>Mary                        </td><td>female</td><td>third</td><td> 23275</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A data.frame: 6 × 9\n",
              "\\begin{tabular}{r|lllllllll}\n",
              "  & filename & id & language & date & author & title & gender & person & length\\\\\n",
              "  & <chr> & <int> & <chr> & <int> & <chr> & <chr> & <chr> & <chr> & <int>\\\\\n",
              "\\hline\n",
              "\t1 & EN\\_1771\\_Mackenzie,Henry\\_TheManofFeeling\\_Novel.txt              & 151 & English & 1771 & Mackenzie,Henry     & TheManofFeeling              & male   & first &  36458\\\\\n",
              "\t2 & EN\\_1771\\_Smollett,Tobias\\_TheExpedictionofHenryClinker\\_Novel.txt & 152 & English & 1771 & Smollett,Tobias     & TheExpedictionofHenryClinker & male   & first & 148261\\\\\n",
              "\t3 & EN\\_1778\\_Burney,Fanny\\_Evelina\\_Novel.txt                         & 153 & English & 1778 & Burney,Fanny        & Evelina                      & female & first & 154168\\\\\n",
              "\t4 & EN\\_1782\\_Burney,Fanny\\_Cecilia\\_Novel.txt                         & 154 & English & 1782 & Burney,Fanny        & Cecilia                      & female & third & 328981\\\\\n",
              "\t5 & EN\\_1786\\_Beckford,William\\_Vathek\\_Novel.txt                      & 155 & English & 1786 & Beckford,William    & Vathek                       & male   & third &  36077\\\\\n",
              "\t6 & EN\\_1788\\_Wollstonecraft,Mary\\_Mary\\_Novel.txt                     & 156 & English & 1788 & Wollstonecraft,Mary & Mary                         & female & third &  23275\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A data.frame: 6 × 9\n",
              "\n",
              "| <!--/--> | filename &lt;chr&gt; | id &lt;int&gt; | language &lt;chr&gt; | date &lt;int&gt; | author &lt;chr&gt; | title &lt;chr&gt; | gender &lt;chr&gt; | person &lt;chr&gt; | length &lt;int&gt; |\n",
              "|---|---|---|---|---|---|---|---|---|---|\n",
              "| 1 | EN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt              | 151 | English | 1771 | Mackenzie,Henry     | TheManofFeeling              | male   | first |  36458 |\n",
              "| 2 | EN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt | 152 | English | 1771 | Smollett,Tobias     | TheExpedictionofHenryClinker | male   | first | 148261 |\n",
              "| 3 | EN_1778_Burney,Fanny_Evelina_Novel.txt                         | 153 | English | 1778 | Burney,Fanny        | Evelina                      | female | first | 154168 |\n",
              "| 4 | EN_1782_Burney,Fanny_Cecilia_Novel.txt                         | 154 | English | 1782 | Burney,Fanny        | Cecilia                      | female | third | 328981 |\n",
              "| 5 | EN_1786_Beckford,William_Vathek_Novel.txt                      | 155 | English | 1786 | Beckford,William    | Vathek                       | male   | third |  36077 |\n",
              "| 6 | EN_1788_Wollstonecraft,Mary_Mary_Novel.txt                     | 156 | English | 1788 | Wollstonecraft,Mary | Mary                         | female | third |  23275 |\n",
              "\n"
            ],
            "text/plain": [
              "  filename                                                       id  language\n",
              "1 EN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt              151 English \n",
              "2 EN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt 152 English \n",
              "3 EN_1778_Burney,Fanny_Evelina_Novel.txt                         153 English \n",
              "4 EN_1782_Burney,Fanny_Cecilia_Novel.txt                         154 English \n",
              "5 EN_1786_Beckford,William_Vathek_Novel.txt                      155 English \n",
              "6 EN_1788_Wollstonecraft,Mary_Mary_Novel.txt                     156 English \n",
              "  date author              title                        gender person length\n",
              "1 1771 Mackenzie,Henry     TheManofFeeling              male   first   36458\n",
              "2 1771 Smollett,Tobias     TheExpedictionofHenryClinker male   first  148261\n",
              "3 1778 Burney,Fanny        Evelina                      female first  154168\n",
              "4 1782 Burney,Fanny        Cecilia                      female third  328981\n",
              "5 1786 Beckford,William    Vathek                       male   third   36077\n",
              "6 1788 Wollstonecraft,Mary Mary                         female third   23275"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check Metadata\n",
        "head(meta_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2WG311mh6yG"
      },
      "source": [
        "#### Import Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "t8lk5nkch6yG"
      },
      "outputs": [],
      "source": [
        "# Set the path to the 'fiction_folder'\n",
        "fiction_folder <- \"txtlab_Novel450_English/\"\n",
        "\n",
        "# Create a list to store the file paths\n",
        "file_paths <- list.files(fiction_folder, full.names = TRUE)\n",
        "\n",
        "# Read all the files as a list of single strings\n",
        "novel_list <- lapply(file_paths, function(filepath) {\n",
        "  readChar(filepath, file.info(filepath)$size)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LaETvuufh6yG",
        "outputId": "a00bceec-f066-472c-e66a-5fbec9e4a789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CASTLE RACKRENT\n",
            "\n",
            "by Maria Edgeworth\n",
            "\n",
            "\n",
            "\n",
            "AUTHOR'S PREFACE\n",
            "\n",
            "The Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de"
          ]
        }
      ],
      "source": [
        "# Inspect first item in novel_list\n",
        "cat(substr(novel_list[[1]], 1, 500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSXsvqdqh6yG"
      },
      "source": [
        "#### Pre-Processing\n",
        "Word2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we  want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\n",
        "\n",
        "Since novels were imported as single strings, we'll first need to divide them into sentences, and second, we'll split each sentence into its own list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm07UWenh6yG"
      },
      "outputs": [],
      "source": [
        "# Define a regular expression pattern for sentence splitting\n",
        "sentence_pattern <- \"[^.!?]+(?<!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n",
        "\n",
        "# Split each novel into sentences\n",
        "sentences <- unlist(lapply(novel_list, function(novel) {\n",
        "  str_extract_all(novel, sentence_pattern)[[1]]\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzCQS7ZTh6yH",
        "outputId": "a243263a-7787-4a03-e3fc-863a613db3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n"
          ]
        }
      ],
      "source": [
        "first_sentence <- sentences[1]\n",
        "print(first_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObCnDZf3h6yH"
      },
      "source": [
        "We are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTxkyoNhh6yH",
        "outputId": "1391e8fb-decb-4a3a-f359-f17cab658b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n"
          ]
        }
      ],
      "source": [
        "fast_tokenize <- function(text) {\n",
        "\n",
        "  # Remove punctuation characters\n",
        "  no_punct <- gsub(\"[[:punct:]]\", \"\", tolower(text))\n",
        "\n",
        "  # Split text over whitespace into a character vector of words\n",
        "  tokens <- strsplit(no_punct, \"\\\\s+\")[[1]]\n",
        "\n",
        "  return(tokens)\n",
        "}\n",
        "\n",
        "# Example usage\n",
        "text <- \"Hello, world! This is an example sentence.\"\n",
        "tokens <- fast_tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC0GWK34h6yH"
      },
      "outputs": [],
      "source": [
        "# Time: 2 mins\n",
        "# Split each sentence into tokens\n",
        "# this will take 1-2 minutes\n",
        "\n",
        "words_by_sentence <- lapply(sentences, function(sentence) {\n",
        "  fast_tokenize(sentence)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zRRx6R0h6yH"
      },
      "outputs": [],
      "source": [
        "# Remove any sentences that contain zero tokens\n",
        "words_by_sentence <- words_by_sentence[sapply(words_by_sentence, length) > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bv7VhCGkh6yH",
        "outputId": "29f1e6fc-b6e3-4736-b942-db7cf69b2f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n",
            " [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n",
            " [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n",
            "[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n",
            "[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n",
            "[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n",
            "[25] \"to\"            \"the\"           \"character\"     \"of\"           \n",
            "[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n",
            "[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n",
            "[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n",
            "[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n",
            "[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n",
            "[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n",
            "[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n",
            "[57] \"the\"           \"present\"       \"times\"        \n"
          ]
        }
      ],
      "source": [
        "# Inspect first sentence\n",
        "\n",
        "first_sentence_tokens <- words_by_sentence[[1]]\n",
        "print(first_sentence_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZR-Q2L3h6yH"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train the model we can use this code\n",
        "\n",
        "```{r}\n",
        "# Time: 3 mins\n",
        "# Train word2vec model from txtLab corpus\n",
        "\n",
        "model <- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\n",
        "```\n",
        "\n",
        "However, this is both very slow and very memory instensive.  Instead, we will short-cut here to load the saved results instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGThhDUIh6yH"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model word2vec model from txtLab corpus\n",
        "model <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n",
        "model$wv <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wn9addh6yH"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "> Note: the output here is different than the Python version, even though the model is using the same parameters and same input, which is *sentences*\n",
        "\n",
        "This create a 100-dimension representation of specific words in the text corpus.  This is a _dense_ vector, meaning all of the valaues are (usually) non-zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "N2Ud3z-fh6yI",
        "outputId": "7b146669-49ad-448c-cd63-3c4e563c5db3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 100 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>dimension</th><th scope=col>value</th></tr>\n",
              "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td> 1</td><td>-0.55107111</td></tr>\n",
              "\t<tr><td> 2</td><td>-0.11189298</td></tr>\n",
              "\t<tr><td> 3</td><td>-0.04959059</td></tr>\n",
              "\t<tr><td> 4</td><td>-0.05850497</td></tr>\n",
              "\t<tr><td> 5</td><td> 0.28790763</td></tr>\n",
              "\t<tr><td> 6</td><td>-0.80342406</td></tr>\n",
              "\t<tr><td> 7</td><td>-0.07215538</td></tr>\n",
              "\t<tr><td> 8</td><td> 0.27215561</td></tr>\n",
              "\t<tr><td> 9</td><td>-0.24760762</td></tr>\n",
              "\t<tr><td>10</td><td>-0.40519261</td></tr>\n",
              "\t<tr><td>11</td><td> 0.01354405</td></tr>\n",
              "\t<tr><td>12</td><td>-0.71650523</td></tr>\n",
              "\t<tr><td>13</td><td> 0.17665575</td></tr>\n",
              "\t<tr><td>14</td><td> 0.40048674</td></tr>\n",
              "\t<tr><td>15</td><td>-0.19900815</td></tr>\n",
              "\t<tr><td>16</td><td> 0.20170024</td></tr>\n",
              "\t<tr><td>17</td><td> 0.26689592</td></tr>\n",
              "\t<tr><td>18</td><td>-0.07850418</td></tr>\n",
              "\t<tr><td>19</td><td> 0.41761532</td></tr>\n",
              "\t<tr><td>20</td><td>-0.46563399</td></tr>\n",
              "\t<tr><td>21</td><td>-0.02264982</td></tr>\n",
              "\t<tr><td>22</td><td> 0.03582832</td></tr>\n",
              "\t<tr><td>23</td><td>-0.39578339</td></tr>\n",
              "\t<tr><td>24</td><td>-0.35047379</td></tr>\n",
              "\t<tr><td>25</td><td>-0.10894601</td></tr>\n",
              "\t<tr><td>26</td><td>-0.02075713</td></tr>\n",
              "\t<tr><td>27</td><td>-0.08951025</td></tr>\n",
              "\t<tr><td>28</td><td> 0.63399905</td></tr>\n",
              "\t<tr><td>29</td><td>-0.22439238</td></tr>\n",
              "\t<tr><td>30</td><td>-0.04571422</td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td> 71</td><td>-0.411359429</td></tr>\n",
              "\t<tr><td> 72</td><td> 0.792344272</td></tr>\n",
              "\t<tr><td> 73</td><td>-0.200834081</td></tr>\n",
              "\t<tr><td> 74</td><td> 0.008296484</td></tr>\n",
              "\t<tr><td> 75</td><td> 0.292287439</td></tr>\n",
              "\t<tr><td> 76</td><td>-0.082145669</td></tr>\n",
              "\t<tr><td> 77</td><td> 0.632542729</td></tr>\n",
              "\t<tr><td> 78</td><td>-0.288833410</td></tr>\n",
              "\t<tr><td> 79</td><td>-0.185351834</td></tr>\n",
              "\t<tr><td> 80</td><td> 0.623070717</td></tr>\n",
              "\t<tr><td> 81</td><td>-0.233284771</td></tr>\n",
              "\t<tr><td> 82</td><td> 0.187108710</td></tr>\n",
              "\t<tr><td> 83</td><td>-0.454192758</td></tr>\n",
              "\t<tr><td> 84</td><td> 0.260974020</td></tr>\n",
              "\t<tr><td> 85</td><td>-0.324976146</td></tr>\n",
              "\t<tr><td> 86</td><td> 0.066707216</td></tr>\n",
              "\t<tr><td> 87</td><td> 0.081604123</td></tr>\n",
              "\t<tr><td> 88</td><td> 0.432761550</td></tr>\n",
              "\t<tr><td> 89</td><td> 0.345049858</td></tr>\n",
              "\t<tr><td> 90</td><td> 0.445526332</td></tr>\n",
              "\t<tr><td> 91</td><td> 0.613026440</td></tr>\n",
              "\t<tr><td> 92</td><td>-0.091125637</td></tr>\n",
              "\t<tr><td> 93</td><td> 0.102697603</td></tr>\n",
              "\t<tr><td> 94</td><td> 0.083106160</td></tr>\n",
              "\t<tr><td> 95</td><td> 0.331327826</td></tr>\n",
              "\t<tr><td> 96</td><td> 0.235871971</td></tr>\n",
              "\t<tr><td> 97</td><td>-0.039666425</td></tr>\n",
              "\t<tr><td> 98</td><td> 0.034904104</td></tr>\n",
              "\t<tr><td> 99</td><td> 0.068354718</td></tr>\n",
              "\t<tr><td>100</td><td> 0.002279866</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A data.frame: 100 × 2\n",
              "\\begin{tabular}{ll}\n",
              " dimension & value\\\\\n",
              " <int> & <dbl>\\\\\n",
              "\\hline\n",
              "\t  1 & -0.55107111\\\\\n",
              "\t  2 & -0.11189298\\\\\n",
              "\t  3 & -0.04959059\\\\\n",
              "\t  4 & -0.05850497\\\\\n",
              "\t  5 &  0.28790763\\\\\n",
              "\t  6 & -0.80342406\\\\\n",
              "\t  7 & -0.07215538\\\\\n",
              "\t  8 &  0.27215561\\\\\n",
              "\t  9 & -0.24760762\\\\\n",
              "\t 10 & -0.40519261\\\\\n",
              "\t 11 &  0.01354405\\\\\n",
              "\t 12 & -0.71650523\\\\\n",
              "\t 13 &  0.17665575\\\\\n",
              "\t 14 &  0.40048674\\\\\n",
              "\t 15 & -0.19900815\\\\\n",
              "\t 16 &  0.20170024\\\\\n",
              "\t 17 &  0.26689592\\\\\n",
              "\t 18 & -0.07850418\\\\\n",
              "\t 19 &  0.41761532\\\\\n",
              "\t 20 & -0.46563399\\\\\n",
              "\t 21 & -0.02264982\\\\\n",
              "\t 22 &  0.03582832\\\\\n",
              "\t 23 & -0.39578339\\\\\n",
              "\t 24 & -0.35047379\\\\\n",
              "\t 25 & -0.10894601\\\\\n",
              "\t 26 & -0.02075713\\\\\n",
              "\t 27 & -0.08951025\\\\\n",
              "\t 28 &  0.63399905\\\\\n",
              "\t 29 & -0.22439238\\\\\n",
              "\t 30 & -0.04571422\\\\\n",
              "\t ⋮ & ⋮\\\\\n",
              "\t  71 & -0.411359429\\\\\n",
              "\t  72 &  0.792344272\\\\\n",
              "\t  73 & -0.200834081\\\\\n",
              "\t  74 &  0.008296484\\\\\n",
              "\t  75 &  0.292287439\\\\\n",
              "\t  76 & -0.082145669\\\\\n",
              "\t  77 &  0.632542729\\\\\n",
              "\t  78 & -0.288833410\\\\\n",
              "\t  79 & -0.185351834\\\\\n",
              "\t  80 &  0.623070717\\\\\n",
              "\t  81 & -0.233284771\\\\\n",
              "\t  82 &  0.187108710\\\\\n",
              "\t  83 & -0.454192758\\\\\n",
              "\t  84 &  0.260974020\\\\\n",
              "\t  85 & -0.324976146\\\\\n",
              "\t  86 &  0.066707216\\\\\n",
              "\t  87 &  0.081604123\\\\\n",
              "\t  88 &  0.432761550\\\\\n",
              "\t  89 &  0.345049858\\\\\n",
              "\t  90 &  0.445526332\\\\\n",
              "\t  91 &  0.613026440\\\\\n",
              "\t  92 & -0.091125637\\\\\n",
              "\t  93 &  0.102697603\\\\\n",
              "\t  94 &  0.083106160\\\\\n",
              "\t  95 &  0.331327826\\\\\n",
              "\t  96 &  0.235871971\\\\\n",
              "\t  97 & -0.039666425\\\\\n",
              "\t  98 &  0.034904104\\\\\n",
              "\t  99 &  0.068354718\\\\\n",
              "\t 100 &  0.002279866\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A data.frame: 100 × 2\n",
              "\n",
              "| dimension &lt;int&gt; | value &lt;dbl&gt; |\n",
              "|---|---|\n",
              "|  1 | -0.55107111 |\n",
              "|  2 | -0.11189298 |\n",
              "|  3 | -0.04959059 |\n",
              "|  4 | -0.05850497 |\n",
              "|  5 |  0.28790763 |\n",
              "|  6 | -0.80342406 |\n",
              "|  7 | -0.07215538 |\n",
              "|  8 |  0.27215561 |\n",
              "|  9 | -0.24760762 |\n",
              "| 10 | -0.40519261 |\n",
              "| 11 |  0.01354405 |\n",
              "| 12 | -0.71650523 |\n",
              "| 13 |  0.17665575 |\n",
              "| 14 |  0.40048674 |\n",
              "| 15 | -0.19900815 |\n",
              "| 16 |  0.20170024 |\n",
              "| 17 |  0.26689592 |\n",
              "| 18 | -0.07850418 |\n",
              "| 19 |  0.41761532 |\n",
              "| 20 | -0.46563399 |\n",
              "| 21 | -0.02264982 |\n",
              "| 22 |  0.03582832 |\n",
              "| 23 | -0.39578339 |\n",
              "| 24 | -0.35047379 |\n",
              "| 25 | -0.10894601 |\n",
              "| 26 | -0.02075713 |\n",
              "| 27 | -0.08951025 |\n",
              "| 28 |  0.63399905 |\n",
              "| 29 | -0.22439238 |\n",
              "| 30 | -0.04571422 |\n",
              "| ⋮ | ⋮ |\n",
              "|  71 | -0.411359429 |\n",
              "|  72 |  0.792344272 |\n",
              "|  73 | -0.200834081 |\n",
              "|  74 |  0.008296484 |\n",
              "|  75 |  0.292287439 |\n",
              "|  76 | -0.082145669 |\n",
              "|  77 |  0.632542729 |\n",
              "|  78 | -0.288833410 |\n",
              "|  79 | -0.185351834 |\n",
              "|  80 |  0.623070717 |\n",
              "|  81 | -0.233284771 |\n",
              "|  82 |  0.187108710 |\n",
              "|  83 | -0.454192758 |\n",
              "|  84 |  0.260974020 |\n",
              "|  85 | -0.324976146 |\n",
              "|  86 |  0.066707216 |\n",
              "|  87 |  0.081604123 |\n",
              "|  88 |  0.432761550 |\n",
              "|  89 |  0.345049858 |\n",
              "|  90 |  0.445526332 |\n",
              "|  91 |  0.613026440 |\n",
              "|  92 | -0.091125637 |\n",
              "|  93 |  0.102697603 |\n",
              "|  94 |  0.083106160 |\n",
              "|  95 |  0.331327826 |\n",
              "|  96 |  0.235871971 |\n",
              "|  97 | -0.039666425 |\n",
              "|  98 |  0.034904104 |\n",
              "|  99 |  0.068354718 |\n",
              "| 100 |  0.002279866 |\n",
              "\n"
            ],
            "text/plain": [
              "    dimension value       \n",
              "1    1        -0.55107111 \n",
              "2    2        -0.11189298 \n",
              "3    3        -0.04959059 \n",
              "4    4        -0.05850497 \n",
              "5    5         0.28790763 \n",
              "6    6        -0.80342406 \n",
              "7    7        -0.07215538 \n",
              "8    8         0.27215561 \n",
              "9    9        -0.24760762 \n",
              "10  10        -0.40519261 \n",
              "11  11         0.01354405 \n",
              "12  12        -0.71650523 \n",
              "13  13         0.17665575 \n",
              "14  14         0.40048674 \n",
              "15  15        -0.19900815 \n",
              "16  16         0.20170024 \n",
              "17  17         0.26689592 \n",
              "18  18        -0.07850418 \n",
              "19  19         0.41761532 \n",
              "20  20        -0.46563399 \n",
              "21  21        -0.02264982 \n",
              "22  22         0.03582832 \n",
              "23  23        -0.39578339 \n",
              "24  24        -0.35047379 \n",
              "25  25        -0.10894601 \n",
              "26  26        -0.02075713 \n",
              "27  27        -0.08951025 \n",
              "28  28         0.63399905 \n",
              "29  29        -0.22439238 \n",
              "30  30        -0.04571422 \n",
              "⋮   ⋮         ⋮           \n",
              "71   71       -0.411359429\n",
              "72   72        0.792344272\n",
              "73   73       -0.200834081\n",
              "74   74        0.008296484\n",
              "75   75        0.292287439\n",
              "76   76       -0.082145669\n",
              "77   77        0.632542729\n",
              "78   78       -0.288833410\n",
              "79   79       -0.185351834\n",
              "80   80        0.623070717\n",
              "81   81       -0.233284771\n",
              "82   82        0.187108710\n",
              "83   83       -0.454192758\n",
              "84   84        0.260974020\n",
              "85   85       -0.324976146\n",
              "86   86        0.066707216\n",
              "87   87        0.081604123\n",
              "88   88        0.432761550\n",
              "89   89        0.345049858\n",
              "90   90        0.445526332\n",
              "91   91        0.613026440\n",
              "92   92       -0.091125637\n",
              "93   93        0.102697603\n",
              "94   94        0.083106160\n",
              "95   95        0.331327826\n",
              "96   96        0.235871971\n",
              "97   97       -0.039666425\n",
              "98   98        0.034904104\n",
              "99   99        0.068354718\n",
              "100 100        0.002279866"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Return dense word vector\n",
        "vector <- model$wv$get_vector(\"whale\")\n",
        "\n",
        "data.frame(dimension = 1:100, value = vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP0bADG6h6yI"
      },
      "source": [
        "## Vector-Space Operations\n",
        "\n",
        "The key advantage of the word-embedding is the dense vector representations of words: these allow us to do _operations_ on those words, which are informative for learning about how those words are used.\n",
        "\n",
        "* This is also where the connection with LLM is created: they use these vectors to inform _predictions_ about sequences of words (and sentences, in more complex models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3tk-Ydh6yI"
      },
      "source": [
        "### Similarity\n",
        "Since words are represented as dense vectors, we can ask how similiar words' meanings are based on their cosine similarity (essentially how much they overlap). <em>gensim</em> has a few out-of-the-box functions that enable different kinds of comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlzgUBggh6yI",
        "outputId": "e24ce90e-b75f-4b53-9c3a-f54a4d47faf0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "0.59162300825119"
            ],
            "text/latex": [
              "0.59162300825119"
            ],
            "text/markdown": [
              "0.59162300825119"
            ],
            "text/plain": [
              "[1] 0.591623"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Find cosine distance between two given word vectors\n",
        "\n",
        "similarity <- model$wv$similarity(\"pride\", \"prejudice\")\n",
        "similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LlFECfgh6yI",
        "outputId": "272604e2-aaa2-40b8-e49e-4e58f04d4cb6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'unworthiness'</li>\n",
              "\t<li>0.708338558673859</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'vanity'</li>\n",
              "\t<li>0.70763099193573</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'hardihood'</li>\n",
              "\t<li>0.703833639621735</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'heroism'</li>\n",
              "\t<li>0.702945291996002</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'selfishness'</li>\n",
              "\t<li>0.69848620891571</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'egotism'</li>\n",
              "\t<li>0.698321938514709</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'unselfishness'</li>\n",
              "\t<li>0.69433867931366</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'arrogance'</li>\n",
              "\t<li>0.693523764610291</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'selfconceit'</li>\n",
              "\t<li>0.690157413482666</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'timidity'</li>\n",
              "\t<li>0.69000643491745</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'unworthiness'\n",
              "\\item 0.708338558673859\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'vanity'\n",
              "\\item 0.70763099193573\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hardihood'\n",
              "\\item 0.703833639621735\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'heroism'\n",
              "\\item 0.702945291996002\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'selfishness'\n",
              "\\item 0.69848620891571\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'egotism'\n",
              "\\item 0.698321938514709\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'unselfishness'\n",
              "\\item 0.69433867931366\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'arrogance'\n",
              "\\item 0.693523764610291\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'selfconceit'\n",
              "\\item 0.690157413482666\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'timidity'\n",
              "\\item 0.69000643491745\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'unworthiness'\n",
              "2. 0.708338558673859\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'vanity'\n",
              "2. 0.70763099193573\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'hardihood'\n",
              "2. 0.703833639621735\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'heroism'\n",
              "2. 0.702945291996002\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'selfishness'\n",
              "2. 0.69848620891571\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'egotism'\n",
              "2. 0.698321938514709\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'unselfishness'\n",
              "2. 0.69433867931366\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'arrogance'\n",
              "2. 0.693523764610291\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'selfconceit'\n",
              "2. 0.690157413482666\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'timidity'\n",
              "2. 0.69000643491745\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"unworthiness\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7083386\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"vanity\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.707631\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"hardihood\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.7038336\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"heroism\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.7029453\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"selfishness\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6984862\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"egotism\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6983219\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"unselfishness\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6943387\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"arrogance\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6935238\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"selfconceit\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6901574\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"timidity\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.6900064\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Find nearest word vectors by cosine distance\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"pride\")\n",
        "most_similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxs_mY6h6yJ",
        "outputId": "323fb458-b53b-40d7-dc5f-587da0194a92"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "'whale'"
            ],
            "text/latex": [
              "'whale'"
            ],
            "text/markdown": [
              "'whale'"
            ],
            "text/plain": [
              "[1] \"whale\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Given a list of words, we can ask which doesn't belong\n",
        "\n",
        "# Finds mean vector of words in list\n",
        "# and identifies the word further from that mean\n",
        "\n",
        "doesnt_match <- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\n",
        "doesnt_match\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLNOcB5gh6yJ"
      },
      "source": [
        "## Multiple Valences\n",
        "A word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of <em>river bank</em> from the word <em>bank</em>. This would be written mathetmatically as <em>RIVER - BANK</em>, which in <em>gensim</em>'s interface lists <em>RIVER</em> as a positive meaning and <em>BANK</em> as a negative one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haT_LCpwh6yJ",
        "outputId": "71f35b48-6036-4b70-e280-0183e05bb203"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'river'</li>\n",
              "\t<li>0.711162984371185</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'creek'</li>\n",
              "\t<li>0.68317973613739</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'shore'</li>\n",
              "\t<li>0.676563084125519</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cove'</li>\n",
              "\t<li>0.675664663314819</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'ferryboat'</li>\n",
              "\t<li>0.671000003814697</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'thames'</li>\n",
              "\t<li>0.669983685016632</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'margin'</li>\n",
              "\t<li>0.669341504573822</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'banks'</li>\n",
              "\t<li>0.665800094604492</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'hanger'</li>\n",
              "\t<li>0.663011133670807</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'wharf'</li>\n",
              "\t<li>0.660356998443604</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'river'\n",
              "\\item 0.711162984371185\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'creek'\n",
              "\\item 0.68317973613739\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'shore'\n",
              "\\item 0.676563084125519\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cove'\n",
              "\\item 0.675664663314819\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'ferryboat'\n",
              "\\item 0.671000003814697\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'thames'\n",
              "\\item 0.669983685016632\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'margin'\n",
              "\\item 0.669341504573822\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'banks'\n",
              "\\item 0.665800094604492\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hanger'\n",
              "\\item 0.663011133670807\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'wharf'\n",
              "\\item 0.660356998443604\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'river'\n",
              "2. 0.711162984371185\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'creek'\n",
              "2. 0.68317973613739\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'shore'\n",
              "2. 0.676563084125519\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'cove'\n",
              "2. 0.675664663314819\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'ferryboat'\n",
              "2. 0.671000003814697\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'thames'\n",
              "2. 0.669983685016632\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'margin'\n",
              "2. 0.669341504573822\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'banks'\n",
              "2. 0.665800094604492\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'hanger'\n",
              "2. 0.663011133670807\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'wharf'\n",
              "2. 0.660356998443604\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"river\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.711163\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"creek\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.6831797\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"shore\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.6765631\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"cove\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.6756647\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"ferryboat\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.671\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"thames\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6699837\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"margin\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6693415\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"banks\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6658001\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"hanger\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6630111\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"wharf\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.660357\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get most similar words to BANK, in order\n",
        "# to get a sense for its primary meaning\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"bank\")\n",
        "most_similar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bupxmp4Oh6yJ",
        "outputId": "c7ace3db-2af6-4e93-a434-68eba02de68a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'unpaid'</li>\n",
              "\t<li>0.373251676559448</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'fee'</li>\n",
              "\t<li>0.370019376277924</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cheque'</li>\n",
              "\t<li>0.359555840492249</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'embezzlement'</li>\n",
              "\t<li>0.357363700866699</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'deposit'</li>\n",
              "\t<li>0.351018667221069</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'salary'</li>\n",
              "\t<li>0.35058805346489</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cash'</li>\n",
              "\t<li>0.350180208683014</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'mortgage'</li>\n",
              "\t<li>0.34438681602478</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cowperwoods'</li>\n",
              "\t<li>0.344247430562973</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'purchase'</li>\n",
              "\t<li>0.342276871204376</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'unpaid'\n",
              "\\item 0.373251676559448\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'fee'\n",
              "\\item 0.370019376277924\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cheque'\n",
              "\\item 0.359555840492249\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'embezzlement'\n",
              "\\item 0.357363700866699\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'deposit'\n",
              "\\item 0.351018667221069\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'salary'\n",
              "\\item 0.35058805346489\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cash'\n",
              "\\item 0.350180208683014\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'mortgage'\n",
              "\\item 0.34438681602478\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cowperwoods'\n",
              "\\item 0.344247430562973\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'purchase'\n",
              "\\item 0.342276871204376\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'unpaid'\n",
              "2. 0.373251676559448\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'fee'\n",
              "2. 0.370019376277924\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'cheque'\n",
              "2. 0.359555840492249\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'embezzlement'\n",
              "2. 0.357363700866699\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'deposit'\n",
              "2. 0.351018667221069\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'salary'\n",
              "2. 0.35058805346489\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'cash'\n",
              "2. 0.350180208683014\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'mortgage'\n",
              "2. 0.34438681602478\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'cowperwoods'\n",
              "2. 0.344247430562973\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'purchase'\n",
              "2. 0.342276871204376\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"unpaid\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.3732517\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"fee\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.3700194\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"cheque\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.3595558\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"embezzlement\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.3573637\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"deposit\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.3510187\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"salary\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.3505881\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"cash\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.3501802\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"mortgage\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.3443868\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"cowperwoods\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.3442474\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"purchase\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.3422769\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Remove the sense of \"river bank\" from \"bank\" and see what is left\n",
        "\n",
        "result <- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaKGAeM_h6yJ"
      },
      "source": [
        "## Analogy\n",
        "Analogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy <em>MAN is to KING as WOMAN is to ??</em> is rendered as <em>KING - MAN + WOMAN</em>. In the gensim interface, we designate <em>KING</em> and <em>WOMAN</em> as positive terms and <em>MAN</em> as a negative term, since it is subtracted from those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1U_D74Oh6yJ",
        "outputId": "f26212d8-f684-490f-a873-f50103474437"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'duke'</li>\n",
              "\t<li>0.795354425907135</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'prince'</li>\n",
              "\t<li>0.745972692966461</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'otho'</li>\n",
              "\t<li>0.7265864610672</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'governor'</li>\n",
              "\t<li>0.714816331863403</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'kings'</li>\n",
              "\t<li>0.695792615413666</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'justicer'</li>\n",
              "\t<li>0.693355023860931</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'commanderinchief'</li>\n",
              "\t<li>0.679358124732971</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'minister'</li>\n",
              "\t<li>0.677222430706024</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'emperor'</li>\n",
              "\t<li>0.669488191604614</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'wizard'</li>\n",
              "\t<li>0.668773353099823</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'duke'\n",
              "\\item 0.795354425907135\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'prince'\n",
              "\\item 0.745972692966461\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'otho'\n",
              "\\item 0.7265864610672\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'governor'\n",
              "\\item 0.714816331863403\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'kings'\n",
              "\\item 0.695792615413666\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'justicer'\n",
              "\\item 0.693355023860931\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'commanderinchief'\n",
              "\\item 0.679358124732971\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'minister'\n",
              "\\item 0.677222430706024\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'emperor'\n",
              "\\item 0.669488191604614\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'wizard'\n",
              "\\item 0.668773353099823\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'duke'\n",
              "2. 0.795354425907135\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'prince'\n",
              "2. 0.745972692966461\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'otho'\n",
              "2. 0.7265864610672\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'governor'\n",
              "2. 0.714816331863403\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'kings'\n",
              "2. 0.695792615413666\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'justicer'\n",
              "2. 0.693355023860931\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'commanderinchief'\n",
              "2. 0.679358124732971\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'minister'\n",
              "2. 0.677222430706024\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'emperor'\n",
              "2. 0.669488191604614\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'wizard'\n",
              "2. 0.668773353099823\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"duke\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7953544\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"prince\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7459727\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"otho\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.7265865\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"governor\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.7148163\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"kings\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6957926\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"justicer\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.693355\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"commanderinchief\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6793581\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"minister\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6772224\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"emperor\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6694882\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"wizard\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.6687734\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get most similar words to KING, in order\n",
        "# to get a sense for its primary meaning\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"king\")\n",
        "most_similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr0NJk4hh6yJ",
        "outputId": "4e74a6c3-4e77-4adf-df32-9ebb0e575c78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'queen'</li>\n",
              "\t<li>0.748667359352112</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'princess'</li>\n",
              "\t<li>0.717491209506989</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'nun'</li>\n",
              "\t<li>0.671820759773254</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'duchess'</li>\n",
              "\t<li>0.663877904415131</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'dunstan'</li>\n",
              "\t<li>0.644907355308533</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'helena'</li>\n",
              "\t<li>0.642244577407837</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'duke'</li>\n",
              "\t<li>0.628719568252563</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'ruritania'</li>\n",
              "\t<li>0.626859545707703</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bride'</li>\n",
              "\t<li>0.622037887573242</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'lomellino'</li>\n",
              "\t<li>0.621977627277374</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'queen'\n",
              "\\item 0.748667359352112\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'princess'\n",
              "\\item 0.717491209506989\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'nun'\n",
              "\\item 0.671820759773254\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'duchess'\n",
              "\\item 0.663877904415131\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'dunstan'\n",
              "\\item 0.644907355308533\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'helena'\n",
              "\\item 0.642244577407837\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'duke'\n",
              "\\item 0.628719568252563\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'ruritania'\n",
              "\\item 0.626859545707703\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bride'\n",
              "\\item 0.622037887573242\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'lomellino'\n",
              "\\item 0.621977627277374\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'queen'\n",
              "2. 0.748667359352112\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'princess'\n",
              "2. 0.717491209506989\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'nun'\n",
              "2. 0.671820759773254\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'duchess'\n",
              "2. 0.663877904415131\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'dunstan'\n",
              "2. 0.644907355308533\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'helena'\n",
              "2. 0.642244577407837\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'duke'\n",
              "2. 0.628719568252563\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'ruritania'\n",
              "2. 0.626859545707703\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'bride'\n",
              "2. 0.622037887573242\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'lomellino'\n",
              "2. 0.621977627277374\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"queen\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7486674\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"princess\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7174912\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"nun\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.6718208\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"duchess\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.6638779\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"dunstan\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6449074\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"helena\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6422446\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"duke\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6287196\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"ruritania\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6268595\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"bride\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6220379\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"lomellino\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.6219776\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The canonic word2vec analogy: King - Man + Woman -> Queen\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOYotlcFh6yK"
      },
      "source": [
        "### Gendered Vectors\n",
        "Can we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nvLa2vgFh6yK",
        "outputId": "5a4cc0a4-271e-4cef-afdb-1648428d926b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'louisa'</li>\n",
              "\t<li>0.50369131565094</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'helens'</li>\n",
              "\t<li>0.457185119390488</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'fragile'</li>\n",
              "\t<li>0.437936186790466</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'maiden'</li>\n",
              "\t<li>0.437387645244598</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'rosabella'</li>\n",
              "\t<li>0.436146855354309</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'jane'</li>\n",
              "\t<li>0.430836617946625</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'anne'</li>\n",
              "\t<li>0.430635213851929</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'charms'</li>\n",
              "\t<li>0.430605590343475</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'elizabeth'</li>\n",
              "\t<li>0.429295152425766</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'womanly'</li>\n",
              "\t<li>0.423216879367828</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'louisa'\n",
              "\\item 0.50369131565094\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'helens'\n",
              "\\item 0.457185119390488\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'fragile'\n",
              "\\item 0.437936186790466\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'maiden'\n",
              "\\item 0.437387645244598\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'rosabella'\n",
              "\\item 0.436146855354309\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'jane'\n",
              "\\item 0.430836617946625\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'anne'\n",
              "\\item 0.430635213851929\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'charms'\n",
              "\\item 0.430605590343475\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'elizabeth'\n",
              "\\item 0.429295152425766\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'womanly'\n",
              "\\item 0.423216879367828\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'louisa'\n",
              "2. 0.50369131565094\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'helens'\n",
              "2. 0.457185119390488\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'fragile'\n",
              "2. 0.437936186790466\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'maiden'\n",
              "2. 0.437387645244598\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'rosabella'\n",
              "2. 0.436146855354309\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'jane'\n",
              "2. 0.430836617946625\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'anne'\n",
              "2. 0.430635213851929\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'charms'\n",
              "2. 0.430605590343475\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'elizabeth'\n",
              "2. 0.429295152425766\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'womanly'\n",
              "2. 0.423216879367828\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"louisa\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.5036913\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"helens\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.4571851\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"fragile\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.4379362\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"maiden\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.4373876\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"rosabella\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.4361469\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"jane\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.4308366\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"anne\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.4306352\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"charms\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.4306056\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"elizabeth\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.4292952\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"womanly\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.4232169\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Feminine Vector\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG26XNQgh6yK",
        "outputId": "2b506916-a722-439a-fa06-9965af0b975a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'mahbub'</li>\n",
              "\t<li>0.42675507068634</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'buck'</li>\n",
              "\t<li>0.401212304830551</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'osterman'</li>\n",
              "\t<li>0.395234704017639</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bicycle'</li>\n",
              "\t<li>0.381052941083908</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bill'</li>\n",
              "\t<li>0.380294442176819</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'policeman'</li>\n",
              "\t<li>0.373987168073654</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'pipe'</li>\n",
              "\t<li>0.366212487220764</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'sergeant'</li>\n",
              "\t<li>0.366210967302322</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'foreman'</li>\n",
              "\t<li>0.359905034303665</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bonneville'</li>\n",
              "\t<li>0.356138646602631</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'mahbub'\n",
              "\\item 0.42675507068634\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'buck'\n",
              "\\item 0.401212304830551\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'osterman'\n",
              "\\item 0.395234704017639\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bicycle'\n",
              "\\item 0.381052941083908\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bill'\n",
              "\\item 0.380294442176819\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'policeman'\n",
              "\\item 0.373987168073654\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'pipe'\n",
              "\\item 0.366212487220764\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'sergeant'\n",
              "\\item 0.366210967302322\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'foreman'\n",
              "\\item 0.359905034303665\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bonneville'\n",
              "\\item 0.356138646602631\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'mahbub'\n",
              "2. 0.42675507068634\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'buck'\n",
              "2. 0.401212304830551\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'osterman'\n",
              "2. 0.395234704017639\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'bicycle'\n",
              "2. 0.381052941083908\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'bill'\n",
              "2. 0.380294442176819\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'policeman'\n",
              "2. 0.373987168073654\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'pipe'\n",
              "2. 0.366212487220764\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'sergeant'\n",
              "2. 0.366210967302322\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'foreman'\n",
              "2. 0.359905034303665\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'bonneville'\n",
              "2. 0.356138646602631\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"mahbub\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.4267551\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"buck\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.4012123\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"osterman\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.3952347\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"bicycle\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.3810529\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"bill\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.3802944\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"policeman\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.3739872\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"pipe\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.3662125\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"sergeant\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.366211\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"foreman\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.359905\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"bonneville\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.3561386\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Masculine Vector\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBATUqLOh6yK"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znja1mKbh6yK",
        "outputId": "284c2f3b-341f-46ca-e2e4-8213727c59d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<dl>\n",
              "\t<dt>[[1]]</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>$the</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>$and</dt>\n",
              "\t\t<dd>2</dd>\n",
              "\t<dt>$of</dt>\n",
              "\t\t<dd>3</dd>\n",
              "\t<dt>$to</dt>\n",
              "\t\t<dd>4</dd>\n",
              "\t<dt>$a</dt>\n",
              "\t\t<dd>5</dd>\n",
              "</dl>\n"
            ],
            "text/latex": [
              "\\begin{description}\n",
              "\\item[{[[1]]}] 0\n",
              "\\item[\\$the] 1\n",
              "\\item[\\$and] 2\n",
              "\\item[\\$of] 3\n",
              "\\item[\\$to] 4\n",
              "\\item[\\$a] 5\n",
              "\\end{description}\n"
            ],
            "text/markdown": [
              "[[1]]\n",
              ":   0\n",
              "$the\n",
              ":   1\n",
              "$and\n",
              ":   2\n",
              "$of\n",
              ":   3\n",
              "$to\n",
              ":   4\n",
              "$a\n",
              ":   5\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[1] 0\n",
              "\n",
              "$the\n",
              "[1] 1\n",
              "\n",
              "$and\n",
              "[1] 2\n",
              "\n",
              "$of\n",
              "[1] 3\n",
              "\n",
              "$to\n",
              "[1] 4\n",
              "\n",
              "$a\n",
              "[1] 5\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Note: due to some discrepencies between Python and R, this may not be translated exactly\n",
        "# Dictionary of words in model\n",
        "\n",
        "key_to_index <- model$wv$key_to_index #this stores the index of each word in the model\n",
        "\n",
        "head(key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aX5NH-vh6yK"
      },
      "outputs": [],
      "source": [
        "# Visualizing the whole vocabulary would make it hard to read\n",
        "\n",
        "key_to_index <- model$wv$key_to_index\n",
        "\n",
        "# Get the number of unique words in the vocabulary (vocabulary size)\n",
        "vocabulary_size <- length(key_to_index)\n",
        "\n",
        "# Find most similar tokens\n",
        "similarity_result <- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n",
        "                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n",
        "                                           topn = as.integer(50))  # Convert to integer\n",
        "\n",
        "# Extract tokens from the result\n",
        "her_tokens <- sapply(similarity_result, function(item) item[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcU_0JEjh6yK",
        "outputId": "9ca63bf8-d71a-477a-cc58-311aec8feeb0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li>'louisa'</li>\n",
              "\t<li>'helens'</li>\n",
              "\t<li>'fragile'</li>\n",
              "\t<li>'maiden'</li>\n",
              "\t<li>'rosabella'</li>\n",
              "\t<li>'jane'</li>\n",
              "\t<li>'anne'</li>\n",
              "\t<li>'charms'</li>\n",
              "\t<li>'elizabeth'</li>\n",
              "\t<li>'womanly'</li>\n",
              "\t<li>'fanny'</li>\n",
              "\t<li>'sex'</li>\n",
              "\t<li>'portmans'</li>\n",
              "\t<li>'lovable'</li>\n",
              "\t<li>'lucy'</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item 'louisa'\n",
              "\\item 'helens'\n",
              "\\item 'fragile'\n",
              "\\item 'maiden'\n",
              "\\item 'rosabella'\n",
              "\\item 'jane'\n",
              "\\item 'anne'\n",
              "\\item 'charms'\n",
              "\\item 'elizabeth'\n",
              "\\item 'womanly'\n",
              "\\item 'fanny'\n",
              "\\item 'sex'\n",
              "\\item 'portmans'\n",
              "\\item 'lovable'\n",
              "\\item 'lucy'\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 'louisa'\n",
              "2. 'helens'\n",
              "3. 'fragile'\n",
              "4. 'maiden'\n",
              "5. 'rosabella'\n",
              "6. 'jane'\n",
              "7. 'anne'\n",
              "8. 'charms'\n",
              "9. 'elizabeth'\n",
              "10. 'womanly'\n",
              "11. 'fanny'\n",
              "12. 'sex'\n",
              "13. 'portmans'\n",
              "14. 'lovable'\n",
              "15. 'lucy'\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[1] \"louisa\"\n",
              "\n",
              "[[2]]\n",
              "[1] \"helens\"\n",
              "\n",
              "[[3]]\n",
              "[1] \"fragile\"\n",
              "\n",
              "[[4]]\n",
              "[1] \"maiden\"\n",
              "\n",
              "[[5]]\n",
              "[1] \"rosabella\"\n",
              "\n",
              "[[6]]\n",
              "[1] \"jane\"\n",
              "\n",
              "[[7]]\n",
              "[1] \"anne\"\n",
              "\n",
              "[[8]]\n",
              "[1] \"charms\"\n",
              "\n",
              "[[9]]\n",
              "[1] \"elizabeth\"\n",
              "\n",
              "[[10]]\n",
              "[1] \"womanly\"\n",
              "\n",
              "[[11]]\n",
              "[1] \"fanny\"\n",
              "\n",
              "[[12]]\n",
              "[1] \"sex\"\n",
              "\n",
              "[[13]]\n",
              "[1] \"portmans\"\n",
              "\n",
              "[[14]]\n",
              "[1] \"lovable\"\n",
              "\n",
              "[[15]]\n",
              "[1] \"lucy\"\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "her_tokens_first_15 <- her_tokens[1:15]\n",
        "\n",
        "# Inspect list\n",
        "her_tokens_first_15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NRkboUu_h6yK",
        "outputId": "796f87ca-7b59-4bb3-8b74-0d77baa3a4ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 5 × 100 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>vectors_matrix</th><td>-0.164372221</td><td>-0.38773462</td><td>-0.2130798</td><td>0.41665018</td><td> 0.02410618</td><td>-0.107125707</td><td> 0.307273984</td><td> 0.28564280</td><td> 0.2715282</td><td>-0.327854037</td><td>⋯</td><td>0.4036772</td><td>0.14915662</td><td>-0.1626078</td><td>0.12744740</td><td>0.01248535</td><td> 0.17607456</td><td> 0.20136480</td><td>0.1819378</td><td> 0.04222544</td><td>-0.24934696</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.065820746</td><td>-0.12796637</td><td>-0.2873217</td><td>0.42668524</td><td>-0.06666858</td><td>-0.007423899</td><td> 0.104129203</td><td> 0.24035919</td><td> 0.3227733</td><td> 0.079232000</td><td>⋯</td><td>0.3743587</td><td>0.18379794</td><td>-0.1555339</td><td>0.11449512</td><td>0.15484981</td><td> 0.31406885</td><td> 0.10346644</td><td>0.1664640</td><td>-0.02670373</td><td> 0.08958896</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.173108056</td><td>-0.23349242</td><td>-0.3337364</td><td>0.52003533</td><td> 0.02572873</td><td> 0.155899152</td><td> 0.015297468</td><td> 0.24247384</td><td> 0.4837905</td><td>-0.271475226</td><td>⋯</td><td>0.4194319</td><td>0.01373206</td><td>-0.2074297</td><td>0.28052822</td><td>0.32765883</td><td> 0.24123329</td><td> 0.05743676</td><td>0.3165979</td><td> 0.05609949</td><td> 0.12584604</td></tr>\n",
              "\t<tr><th scope=row></th><td> 0.008542553</td><td>-0.14920798</td><td>-0.4998134</td><td>0.01760557</td><td>-0.06008903</td><td>-0.137234181</td><td>-0.009309157</td><td> 0.55716687</td><td> 0.2954606</td><td> 0.008222442</td><td>⋯</td><td>0.5059068</td><td>0.10258362</td><td>-0.1766499</td><td>0.08460002</td><td>0.30808648</td><td>-0.07576953</td><td> 0.34530997</td><td>0.2536414</td><td> 0.02444013</td><td> 0.29884085</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.007914007</td><td>-0.03502264</td><td>-0.2320040</td><td>0.21818842</td><td>-0.19631991</td><td>-0.308243692</td><td> 0.335247397</td><td>-0.02742659</td><td>-0.2193182</td><td>-0.375521600</td><td>⋯</td><td>0.5412616</td><td>0.16568597</td><td>-0.6304066</td><td>0.24741997</td><td>0.05831535</td><td> 0.15146731</td><td>-0.10526822</td><td>0.2664579</td><td> 0.18843265</td><td>-0.04899420</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A matrix: 5 × 100 of type dbl\n",
              "\\begin{tabular}{r|lllllllllllllllllllll}\n",
              "\tvectors\\_matrix & -0.164372221 & -0.38773462 & -0.2130798 & 0.41665018 &  0.02410618 & -0.107125707 &  0.307273984 &  0.28564280 &  0.2715282 & -0.327854037 & ⋯ & 0.4036772 & 0.14915662 & -0.1626078 & 0.12744740 & 0.01248535 &  0.17607456 &  0.20136480 & 0.1819378 &  0.04222544 & -0.24934696\\\\\n",
              "\t & -0.065820746 & -0.12796637 & -0.2873217 & 0.42668524 & -0.06666858 & -0.007423899 &  0.104129203 &  0.24035919 &  0.3227733 &  0.079232000 & ⋯ & 0.3743587 & 0.18379794 & -0.1555339 & 0.11449512 & 0.15484981 &  0.31406885 &  0.10346644 & 0.1664640 & -0.02670373 &  0.08958896\\\\\n",
              "\t & -0.173108056 & -0.23349242 & -0.3337364 & 0.52003533 &  0.02572873 &  0.155899152 &  0.015297468 &  0.24247384 &  0.4837905 & -0.271475226 & ⋯ & 0.4194319 & 0.01373206 & -0.2074297 & 0.28052822 & 0.32765883 &  0.24123329 &  0.05743676 & 0.3165979 &  0.05609949 &  0.12584604\\\\\n",
              "\t &  0.008542553 & -0.14920798 & -0.4998134 & 0.01760557 & -0.06008903 & -0.137234181 & -0.009309157 &  0.55716687 &  0.2954606 &  0.008222442 & ⋯ & 0.5059068 & 0.10258362 & -0.1766499 & 0.08460002 & 0.30808648 & -0.07576953 &  0.34530997 & 0.2536414 &  0.02444013 &  0.29884085\\\\\n",
              "\t & -0.007914007 & -0.03502264 & -0.2320040 & 0.21818842 & -0.19631991 & -0.308243692 &  0.335247397 & -0.02742659 & -0.2193182 & -0.375521600 & ⋯ & 0.5412616 & 0.16568597 & -0.6304066 & 0.24741997 & 0.05831535 &  0.15146731 & -0.10526822 & 0.2664579 &  0.18843265 & -0.04899420\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A matrix: 5 × 100 of type dbl\n",
              "\n",
              "| vectors_matrix | -0.164372221 | -0.38773462 | -0.2130798 | 0.41665018 |  0.02410618 | -0.107125707 |  0.307273984 |  0.28564280 |  0.2715282 | -0.327854037 | ⋯ | 0.4036772 | 0.14915662 | -0.1626078 | 0.12744740 | 0.01248535 |  0.17607456 |  0.20136480 | 0.1819378 |  0.04222544 | -0.24934696 |\n",
              "| <!----> | -0.065820746 | -0.12796637 | -0.2873217 | 0.42668524 | -0.06666858 | -0.007423899 |  0.104129203 |  0.24035919 |  0.3227733 |  0.079232000 | ⋯ | 0.3743587 | 0.18379794 | -0.1555339 | 0.11449512 | 0.15484981 |  0.31406885 |  0.10346644 | 0.1664640 | -0.02670373 |  0.08958896 |\n",
              "| <!----> | -0.173108056 | -0.23349242 | -0.3337364 | 0.52003533 |  0.02572873 |  0.155899152 |  0.015297468 |  0.24247384 |  0.4837905 | -0.271475226 | ⋯ | 0.4194319 | 0.01373206 | -0.2074297 | 0.28052822 | 0.32765883 |  0.24123329 |  0.05743676 | 0.3165979 |  0.05609949 |  0.12584604 |\n",
              "| <!----> |  0.008542553 | -0.14920798 | -0.4998134 | 0.01760557 | -0.06008903 | -0.137234181 | -0.009309157 |  0.55716687 |  0.2954606 |  0.008222442 | ⋯ | 0.5059068 | 0.10258362 | -0.1766499 | 0.08460002 | 0.30808648 | -0.07576953 |  0.34530997 | 0.2536414 |  0.02444013 |  0.29884085 |\n",
              "| <!----> | -0.007914007 | -0.03502264 | -0.2320040 | 0.21818842 | -0.19631991 | -0.308243692 |  0.335247397 | -0.02742659 | -0.2193182 | -0.375521600 | ⋯ | 0.5412616 | 0.16568597 | -0.6304066 | 0.24741997 | 0.05831535 |  0.15146731 | -0.10526822 | 0.2664579 |  0.18843265 | -0.04899420 |\n",
              "\n"
            ],
            "text/plain": [
              "               [,1]         [,2]        [,3]       [,4]       [,5]       \n",
              "vectors_matrix -0.164372221 -0.38773462 -0.2130798 0.41665018  0.02410618\n",
              "               -0.065820746 -0.12796637 -0.2873217 0.42668524 -0.06666858\n",
              "               -0.173108056 -0.23349242 -0.3337364 0.52003533  0.02572873\n",
              "                0.008542553 -0.14920798 -0.4998134 0.01760557 -0.06008903\n",
              "               -0.007914007 -0.03502264 -0.2320040 0.21818842 -0.19631991\n",
              "               [,6]         [,7]         [,8]        [,9]       [,10]       \n",
              "vectors_matrix -0.107125707  0.307273984  0.28564280  0.2715282 -0.327854037\n",
              "               -0.007423899  0.104129203  0.24035919  0.3227733  0.079232000\n",
              "                0.155899152  0.015297468  0.24247384  0.4837905 -0.271475226\n",
              "               -0.137234181 -0.009309157  0.55716687  0.2954606  0.008222442\n",
              "               -0.308243692  0.335247397 -0.02742659 -0.2193182 -0.375521600\n",
              "               [,11] [,12]     [,13]      [,14]      [,15]      [,16]     \n",
              "vectors_matrix ⋯     0.4036772 0.14915662 -0.1626078 0.12744740 0.01248535\n",
              "               ⋯     0.3743587 0.18379794 -0.1555339 0.11449512 0.15484981\n",
              "               ⋯     0.4194319 0.01373206 -0.2074297 0.28052822 0.32765883\n",
              "               ⋯     0.5059068 0.10258362 -0.1766499 0.08460002 0.30808648\n",
              "               ⋯     0.5412616 0.16568597 -0.6304066 0.24741997 0.05831535\n",
              "               [,17]       [,18]       [,19]     [,20]       [,21]      \n",
              "vectors_matrix  0.17607456  0.20136480 0.1819378  0.04222544 -0.24934696\n",
              "                0.31406885  0.10346644 0.1664640 -0.02670373  0.08958896\n",
              "                0.24123329  0.05743676 0.3165979  0.05609949  0.12584604\n",
              "               -0.07576953  0.34530997 0.2536414  0.02444013  0.29884085\n",
              "                0.15146731 -0.10526822 0.2664579  0.18843265 -0.04899420"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get the vector for each sampled word\n",
        "\n",
        "for (i in 1:length(her_tokens)){\n",
        "\n",
        "    if (i == 1) { vectors_matrix <- model$wv$get_vector(i) } else {\n",
        "        vectors_matrix <- rbind(vectors_matrix, model$wv$get_vector(i))\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "# Print the vectors matrix\n",
        "head(vectors_matrix, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "O_1pkFcyh6yL",
        "outputId": "ee9be8de-a518-4593-fbdf-5a09eaa0f9ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 5 × 50 of type dbl</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>vectors_matrix</th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col>⋯</th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th><th scope=col></th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>vectors_matrix</th><td>0.0000000</td><td>0.2441613</td><td>0.2900885</td><td>0.4458981</td><td>0.3274742</td><td>0.5704110</td><td>0.3381941</td><td>0.4577147</td><td>0.4616800</td><td>0.3537493</td><td>⋯</td><td>0.4658162</td><td>0.6834794</td><td>0.5143328</td><td>0.6211871</td><td>0.6248492</td><td>0.5446319</td><td>0.4857980</td><td>0.4913255</td><td>0.6659446</td><td>0.5421133</td></tr>\n",
              "\t<tr><th scope=row></th><td>0.2441613</td><td>0.0000000</td><td>0.2846745</td><td>0.3217170</td><td>0.3633957</td><td>0.5425751</td><td>0.2804803</td><td>0.4106063</td><td>0.3684607</td><td>0.3231126</td><td>⋯</td><td>0.5097263</td><td>0.6066301</td><td>0.5668863</td><td>0.6740949</td><td>0.6108461</td><td>0.5825670</td><td>0.4197837</td><td>0.4204383</td><td>0.6546490</td><td>0.4473855</td></tr>\n",
              "\t<tr><th scope=row></th><td>0.2900885</td><td>0.2846745</td><td>0.0000000</td><td>0.4830998</td><td>0.4234558</td><td>0.6048161</td><td>0.3192316</td><td>0.4587693</td><td>0.5530774</td><td>0.3821435</td><td>⋯</td><td>0.5454395</td><td>0.6874075</td><td>0.5782254</td><td>0.6966508</td><td>0.6461577</td><td>0.6198037</td><td>0.5841212</td><td>0.4758906</td><td>0.6951999</td><td>0.5767591</td></tr>\n",
              "\t<tr><th scope=row></th><td>0.4458981</td><td>0.3217170</td><td>0.4830998</td><td>0.0000000</td><td>0.4783881</td><td>0.4822524</td><td>0.4423674</td><td>0.5075588</td><td>0.4510846</td><td>0.3553763</td><td>⋯</td><td>0.5372513</td><td>0.5227030</td><td>0.6263146</td><td>0.5838267</td><td>0.6114745</td><td>0.6201524</td><td>0.4680030</td><td>0.5209680</td><td>0.5027610</td><td>0.5204974</td></tr>\n",
              "\t<tr><th scope=row></th><td>0.3274742</td><td>0.3633957</td><td>0.4234558</td><td>0.4783881</td><td>0.0000000</td><td>0.5708173</td><td>0.4317533</td><td>0.4763607</td><td>0.5012079</td><td>0.3991493</td><td>⋯</td><td>0.4914817</td><td>0.6223874</td><td>0.5817080</td><td>0.6721718</td><td>0.6091934</td><td>0.5512868</td><td>0.5899895</td><td>0.5255242</td><td>0.6948704</td><td>0.5963310</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A matrix: 5 × 50 of type dbl\n",
              "\\begin{tabular}{r|lllllllllllllllllllll}\n",
              "  & vectors\\_matrix &  &  &  &  &  &  &  &  &  & ⋯ &  &  &  &  &  &  &  &  &  & \\\\\n",
              "\\hline\n",
              "\tvectors\\_matrix & 0.0000000 & 0.2441613 & 0.2900885 & 0.4458981 & 0.3274742 & 0.5704110 & 0.3381941 & 0.4577147 & 0.4616800 & 0.3537493 & ⋯ & 0.4658162 & 0.6834794 & 0.5143328 & 0.6211871 & 0.6248492 & 0.5446319 & 0.4857980 & 0.4913255 & 0.6659446 & 0.5421133\\\\\n",
              "\t & 0.2441613 & 0.0000000 & 0.2846745 & 0.3217170 & 0.3633957 & 0.5425751 & 0.2804803 & 0.4106063 & 0.3684607 & 0.3231126 & ⋯ & 0.5097263 & 0.6066301 & 0.5668863 & 0.6740949 & 0.6108461 & 0.5825670 & 0.4197837 & 0.4204383 & 0.6546490 & 0.4473855\\\\\n",
              "\t & 0.2900885 & 0.2846745 & 0.0000000 & 0.4830998 & 0.4234558 & 0.6048161 & 0.3192316 & 0.4587693 & 0.5530774 & 0.3821435 & ⋯ & 0.5454395 & 0.6874075 & 0.5782254 & 0.6966508 & 0.6461577 & 0.6198037 & 0.5841212 & 0.4758906 & 0.6951999 & 0.5767591\\\\\n",
              "\t & 0.4458981 & 0.3217170 & 0.4830998 & 0.0000000 & 0.4783881 & 0.4822524 & 0.4423674 & 0.5075588 & 0.4510846 & 0.3553763 & ⋯ & 0.5372513 & 0.5227030 & 0.6263146 & 0.5838267 & 0.6114745 & 0.6201524 & 0.4680030 & 0.5209680 & 0.5027610 & 0.5204974\\\\\n",
              "\t & 0.3274742 & 0.3633957 & 0.4234558 & 0.4783881 & 0.0000000 & 0.5708173 & 0.4317533 & 0.4763607 & 0.5012079 & 0.3991493 & ⋯ & 0.4914817 & 0.6223874 & 0.5817080 & 0.6721718 & 0.6091934 & 0.5512868 & 0.5899895 & 0.5255242 & 0.6948704 & 0.5963310\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A matrix: 5 × 50 of type dbl\n",
              "\n",
              "| <!--/--> | vectors_matrix | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | ⋯ | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> | <!----> |\n",
              "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
              "| vectors_matrix | 0.0000000 | 0.2441613 | 0.2900885 | 0.4458981 | 0.3274742 | 0.5704110 | 0.3381941 | 0.4577147 | 0.4616800 | 0.3537493 | ⋯ | 0.4658162 | 0.6834794 | 0.5143328 | 0.6211871 | 0.6248492 | 0.5446319 | 0.4857980 | 0.4913255 | 0.6659446 | 0.5421133 |\n",
              "| <!----> | 0.2441613 | 0.0000000 | 0.2846745 | 0.3217170 | 0.3633957 | 0.5425751 | 0.2804803 | 0.4106063 | 0.3684607 | 0.3231126 | ⋯ | 0.5097263 | 0.6066301 | 0.5668863 | 0.6740949 | 0.6108461 | 0.5825670 | 0.4197837 | 0.4204383 | 0.6546490 | 0.4473855 |\n",
              "| <!----> | 0.2900885 | 0.2846745 | 0.0000000 | 0.4830998 | 0.4234558 | 0.6048161 | 0.3192316 | 0.4587693 | 0.5530774 | 0.3821435 | ⋯ | 0.5454395 | 0.6874075 | 0.5782254 | 0.6966508 | 0.6461577 | 0.6198037 | 0.5841212 | 0.4758906 | 0.6951999 | 0.5767591 |\n",
              "| <!----> | 0.4458981 | 0.3217170 | 0.4830998 | 0.0000000 | 0.4783881 | 0.4822524 | 0.4423674 | 0.5075588 | 0.4510846 | 0.3553763 | ⋯ | 0.5372513 | 0.5227030 | 0.6263146 | 0.5838267 | 0.6114745 | 0.6201524 | 0.4680030 | 0.5209680 | 0.5027610 | 0.5204974 |\n",
              "| <!----> | 0.3274742 | 0.3633957 | 0.4234558 | 0.4783881 | 0.0000000 | 0.5708173 | 0.4317533 | 0.4763607 | 0.5012079 | 0.3991493 | ⋯ | 0.4914817 | 0.6223874 | 0.5817080 | 0.6721718 | 0.6091934 | 0.5512868 | 0.5899895 | 0.5255242 | 0.6948704 | 0.5963310 |\n",
              "\n"
            ],
            "text/plain": [
              "               vectors_matrix                                                  \n",
              "vectors_matrix 0.0000000      0.2441613 0.2900885 0.4458981 0.3274742 0.5704110\n",
              "               0.2441613      0.0000000 0.2846745 0.3217170 0.3633957 0.5425751\n",
              "               0.2900885      0.2846745 0.0000000 0.4830998 0.4234558 0.6048161\n",
              "               0.4458981      0.3217170 0.4830998 0.0000000 0.4783881 0.4822524\n",
              "               0.3274742      0.3633957 0.4234558 0.4783881 0.0000000 0.5708173\n",
              "                                                       ⋯                    \n",
              "vectors_matrix 0.3381941 0.4577147 0.4616800 0.3537493 ⋯ 0.4658162 0.6834794\n",
              "               0.2804803 0.4106063 0.3684607 0.3231126 ⋯ 0.5097263 0.6066301\n",
              "               0.3192316 0.4587693 0.5530774 0.3821435 ⋯ 0.5454395 0.6874075\n",
              "               0.4423674 0.5075588 0.4510846 0.3553763 ⋯ 0.5372513 0.5227030\n",
              "               0.4317533 0.4763607 0.5012079 0.3991493 ⋯ 0.4914817 0.6223874\n",
              "                                                                          \n",
              "vectors_matrix 0.5143328 0.6211871 0.6248492 0.5446319 0.4857980 0.4913255\n",
              "               0.5668863 0.6740949 0.6108461 0.5825670 0.4197837 0.4204383\n",
              "               0.5782254 0.6966508 0.6461577 0.6198037 0.5841212 0.4758906\n",
              "               0.6263146 0.5838267 0.6114745 0.6201524 0.4680030 0.5209680\n",
              "               0.5817080 0.6721718 0.6091934 0.5512868 0.5899895 0.5255242\n",
              "                                  \n",
              "vectors_matrix 0.6659446 0.5421133\n",
              "               0.6546490 0.4473855\n",
              "               0.6951999 0.5767591\n",
              "               0.5027610 0.5204974\n",
              "               0.6948704 0.5963310"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Calculate distances among texts in vector space\n",
        "\n",
        "dist_matrix <- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n",
        "\n",
        "# Print the distance matrix\n",
        "head(dist_matrix, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-GYXy_9Eh6yL",
        "outputId": "52030ba3-37bc-46b5-9b7c-4d2144a2c735"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 6 × 2 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>vectors_matrix</th><td>-0.22409723</td><td> 0.08093873</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.21332498</td><td> 0.08532053</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.28188636</td><td>-0.01665760</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.01525136</td><td> 0.05774135</td></tr>\n",
              "\t<tr><th scope=row></th><td>-0.13399081</td><td> 0.05947442</td></tr>\n",
              "\t<tr><th scope=row></th><td> 0.25412585</td><td> 0.07946914</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": [
              "A matrix: 6 × 2 of type dbl\n",
              "\\begin{tabular}{r|ll}\n",
              "\tvectors\\_matrix & -0.22409723 &  0.08093873\\\\\n",
              "\t & -0.21332498 &  0.08532053\\\\\n",
              "\t & -0.28188636 & -0.01665760\\\\\n",
              "\t & -0.01525136 &  0.05774135\\\\\n",
              "\t & -0.13399081 &  0.05947442\\\\\n",
              "\t &  0.25412585 &  0.07946914\\\\\n",
              "\\end{tabular}\n"
            ],
            "text/markdown": [
              "\n",
              "A matrix: 6 × 2 of type dbl\n",
              "\n",
              "| vectors_matrix | -0.22409723 |  0.08093873 |\n",
              "| <!----> | -0.21332498 |  0.08532053 |\n",
              "| <!----> | -0.28188636 | -0.01665760 |\n",
              "| <!----> | -0.01525136 |  0.05774135 |\n",
              "| <!----> | -0.13399081 |  0.05947442 |\n",
              "| <!----> |  0.25412585 |  0.07946914 |\n",
              "\n"
            ],
            "text/plain": [
              "               [,1]        [,2]       \n",
              "vectors_matrix -0.22409723  0.08093873\n",
              "               -0.21332498  0.08532053\n",
              "               -0.28188636 -0.01665760\n",
              "               -0.01525136  0.05774135\n",
              "               -0.13399081  0.05947442\n",
              "                0.25412585  0.07946914"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Multi-Dimensional Scaling (Project vectors into 2-D)\n",
        "\n",
        "\n",
        "# Perform Multi-Dimensional Scaling (MDS)\n",
        "mds <- cmdscale(dist_matrix, k = 2)\n",
        "\n",
        "# Print the resulting MDS embeddings\n",
        "head(mds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUYzAF7Oh6yL",
        "outputId": "d81a5de5-440d-4bed-fff7-47d9eb0e7c30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAwFBMVEUAAAAXFxclJSUuLi4y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"
          },
          "metadata": {
            "image/png": {
              "height": 420,
              "width": 420
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_data <- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n",
        "\n",
        "\n",
        "# Create the scatter plot with text labels using ggplot2\n",
        "p <- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n",
        "  geom_point(alpha = 0) +\n",
        "  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n",
        "  theme_minimal()\n",
        "\n",
        "# Print the plot\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzoHw2Xgh6yL",
        "outputId": "b61b822d-2454-453a-b86c-74d511629c16"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAxlBMVEUAAAAXFxclJSUuLi4y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\ngg=="
          },
          "metadata": {
            "image/png": {
              "height": 420,
              "width": 420
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# For comparison, here is the same graph using a masculine-pronoun vector\n",
        "\n",
        "# Find most similar tokens\n",
        "similarity_result <- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n",
        "                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n",
        "                                           topn = as.integer(50))  # Convert to integer\n",
        "\n",
        "his_tokens <- sapply(similarity_result, function(item) item[1])\n",
        "\n",
        "\n",
        "# Get the vector for each sampled word\n",
        "\n",
        "for (i in 1:length(his_tokens)){\n",
        "\n",
        "    if (i == 1) { vectors_matrix <- model$wv$get_vector(i) } else {\n",
        "        vectors_matrix <- rbind(vectors_matrix, model$wv$get_vector(i))\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "dist_matrix <- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n",
        "\n",
        "mds <- cmdscale(dist_matrix, k = 2)\n",
        "\n",
        "plot_data <- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n",
        "\n",
        "# Create the scatter plot with text labels using ggplot2\n",
        "p <- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n",
        "  geom_point(alpha = 0) +\n",
        "  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n",
        "  theme_minimal()\n",
        "\n",
        "# Print the plot\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0nu-A6bh6yL"
      },
      "source": [
        "> ### <span style=\"color:#CC7A00\" > **Questions:** </p>\n",
        "> <span style=\"color:#CC7A00\" > What kinds of semantic relationships exist in the diagram above? </p>\n",
        "> <span style=\"color:#CC7A00\" > Are there any words that seem out of place?\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kAs_qB4h6yL"
      },
      "source": [
        "## 3. Saving/Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k-wQ0uZh6yL"
      },
      "outputs": [],
      "source": [
        "# Save current model for later use\n",
        "\n",
        "model$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJGwS-_hh6yL"
      },
      "outputs": [],
      "source": [
        "# Load up models from disk\n",
        "\n",
        "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
        "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
        "\n",
        "ecco_model <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1fMRhcDh6yL",
        "outputId": "575c4b65-811b-45f5-9a16-1067bdb4e5d9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'ground'</li>\n",
              "\t<li>0.657000720500946</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'turf'</li>\n",
              "\t<li>0.656409680843353</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'surface'</li>\n",
              "\t<li>0.648072481155396</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'declivity'</li>\n",
              "\t<li>0.642420768737793</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'hill'</li>\n",
              "\t<li>0.637111485004425</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bridge'</li>\n",
              "\t<li>0.633224129676819</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'terrace'</li>\n",
              "\t<li>0.630118608474731</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'channel'</li>\n",
              "\t<li>0.629577100276947</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'banks'</li>\n",
              "\t<li>0.629473924636841</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'wall'</li>\n",
              "\t<li>0.62891036272049</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'ground'\n",
              "\\item 0.657000720500946\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'turf'\n",
              "\\item 0.656409680843353\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'surface'\n",
              "\\item 0.648072481155396\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'declivity'\n",
              "\\item 0.642420768737793\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hill'\n",
              "\\item 0.637111485004425\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bridge'\n",
              "\\item 0.633224129676819\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'terrace'\n",
              "\\item 0.630118608474731\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'channel'\n",
              "\\item 0.629577100276947\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'banks'\n",
              "\\item 0.629473924636841\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'wall'\n",
              "\\item 0.62891036272049\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'ground'\n",
              "2. 0.657000720500946\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'turf'\n",
              "2. 0.656409680843353\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'surface'\n",
              "2. 0.648072481155396\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'declivity'\n",
              "2. 0.642420768737793\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'hill'\n",
              "2. 0.637111485004425\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'bridge'\n",
              "2. 0.633224129676819\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'terrace'\n",
              "2. 0.630118608474731\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'channel'\n",
              "2. 0.629577100276947\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'banks'\n",
              "2. 0.629473924636841\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'wall'\n",
              "2. 0.62891036272049\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"ground\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.6570007\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"turf\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.6564097\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"surface\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.6480725\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"declivity\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.6424208\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"hill\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6371115\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"bridge\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6332241\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"terrace\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6301186\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"channel\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6295771\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"banks\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6294739\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"wall\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.6289104\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# What are similar words to BANK?\n",
        "\n",
        "ecco_model$most_similar('bank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQjHo_xhh6yM",
        "outputId": "0a769dab-0bef-4954-956f-31fa38ce39a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'currency'</li>\n",
              "\t<li>0.367142558097839</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'suit'</li>\n",
              "\t<li>0.359229028224945</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'stamp'</li>\n",
              "\t<li>0.358203798532486</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'promissory'</li>\n",
              "\t<li>0.356053054332733</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'pension'</li>\n",
              "\t<li>0.351832240819931</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'blank'</li>\n",
              "\t<li>0.351817756891251</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'payable'</li>\n",
              "\t<li>0.34270504117012</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'mortality'</li>\n",
              "\t<li>0.342624574899673</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'weekly'</li>\n",
              "\t<li>0.340806037187576</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'weal'</li>\n",
              "\t<li>0.33093598484993</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'currency'\n",
              "\\item 0.367142558097839\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'suit'\n",
              "\\item 0.359229028224945\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'stamp'\n",
              "\\item 0.358203798532486\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'promissory'\n",
              "\\item 0.356053054332733\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'pension'\n",
              "\\item 0.351832240819931\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'blank'\n",
              "\\item 0.351817756891251\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'payable'\n",
              "\\item 0.34270504117012\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'mortality'\n",
              "\\item 0.342624574899673\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'weekly'\n",
              "\\item 0.340806037187576\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'weal'\n",
              "\\item 0.33093598484993\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'currency'\n",
              "2. 0.367142558097839\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'suit'\n",
              "2. 0.359229028224945\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'stamp'\n",
              "2. 0.358203798532486\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'promissory'\n",
              "2. 0.356053054332733\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'pension'\n",
              "2. 0.351832240819931\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'blank'\n",
              "2. 0.351817756891251\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'payable'\n",
              "2. 0.34270504117012\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'mortality'\n",
              "2. 0.342624574899673\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'weekly'\n",
              "2. 0.340806037187576\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'weal'\n",
              "2. 0.33093598484993\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"currency\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.3671426\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"suit\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.359229\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"stamp\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.3582038\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"promissory\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.3560531\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"pension\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.3518322\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"blank\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.3518178\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"payable\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.342705\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"mortality\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.3426246\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"weekly\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.340806\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"weal\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.330936\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# What if we remove the sense of \"river bank\"?\n",
        "ecco_model$most_similar(positive = list('bank'), negative = list('river'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHpBc2ssh6yM"
      },
      "source": [
        "## Exercises!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ30vjWKh6yM"
      },
      "source": [
        "See if you can attempt the following exercises on your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMeNl11mh6yM",
        "outputId": "f722e6e7-9b8c-4b38-e02e-f8aa87511d00"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'hansom'</li>\n",
              "\t<li>0.750069677829742</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'taxi'</li>\n",
              "\t<li>0.747884094715118</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cars'</li>\n",
              "\t<li>0.739487826824188</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'buggy'</li>\n",
              "\t<li>0.737066686153412</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'wagon'</li>\n",
              "\t<li>0.736345946788788</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'motor'</li>\n",
              "\t<li>0.732464134693146</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'omnibus'</li>\n",
              "\t<li>0.727235496044159</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'bus'</li>\n",
              "\t<li>0.718657851219177</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'cab'</li>\n",
              "\t<li>0.711317777633667</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'sled'</li>\n",
              "\t<li>0.704099357128143</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hansom'\n",
              "\\item 0.750069677829742\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'taxi'\n",
              "\\item 0.747884094715118\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cars'\n",
              "\\item 0.739487826824188\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'buggy'\n",
              "\\item 0.737066686153412\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'wagon'\n",
              "\\item 0.736345946788788\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'motor'\n",
              "\\item 0.732464134693146\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'omnibus'\n",
              "\\item 0.727235496044159\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'bus'\n",
              "\\item 0.718657851219177\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'cab'\n",
              "\\item 0.711317777633667\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'sled'\n",
              "\\item 0.704099357128143\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'hansom'\n",
              "2. 0.750069677829742\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'taxi'\n",
              "2. 0.747884094715118\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'cars'\n",
              "2. 0.739487826824188\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'buggy'\n",
              "2. 0.737066686153412\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'wagon'\n",
              "2. 0.736345946788788\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'motor'\n",
              "2. 0.732464134693146\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'omnibus'\n",
              "2. 0.727235496044159\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'bus'\n",
              "2. 0.718657851219177\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'cab'\n",
              "2. 0.711317777633667\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'sled'\n",
              "2. 0.704099357128143\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"hansom\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7500697\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"taxi\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7478841\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"cars\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.7394878\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"buggy\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.7370667\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"wagon\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.7363459\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"motor\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.7324641\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"omnibus\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.7272355\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"bus\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.7186579\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"cab\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.7113178\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"sled\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.7040994\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n",
        "##     Do the same for 'motorcar'.\n",
        "\n",
        "## Q.  What characterizes these two words inthe corpus? Does this make sense?\n",
        "\n",
        "model$wv$most_similar(\"car\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ5rHgM2h6yM",
        "outputId": "fac56b48-88a6-4e44-9467-dec4b8445e49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'haha'</li>\n",
              "\t<li>0.78786438703537</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'laundry'</li>\n",
              "\t<li>0.762444496154785</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'hoop'</li>\n",
              "\t<li>0.762144804000854</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'hallway'</li>\n",
              "\t<li>0.747283399105072</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'taxi'</li>\n",
              "\t<li>0.745568156242371</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'slowed'</li>\n",
              "\t<li>0.743111431598663</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'broom'</li>\n",
              "\t<li>0.740418314933777</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'latchkey'</li>\n",
              "\t<li>0.739296555519104</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'joness'</li>\n",
              "\t<li>0.739260911941528</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'shack'</li>\n",
              "\t<li>0.738708138465881</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'haha'\n",
              "\\item 0.78786438703537\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'laundry'\n",
              "\\item 0.762444496154785\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hoop'\n",
              "\\item 0.762144804000854\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'hallway'\n",
              "\\item 0.747283399105072\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'taxi'\n",
              "\\item 0.745568156242371\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'slowed'\n",
              "\\item 0.743111431598663\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'broom'\n",
              "\\item 0.740418314933777\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'latchkey'\n",
              "\\item 0.739296555519104\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'joness'\n",
              "\\item 0.739260911941528\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'shack'\n",
              "\\item 0.738708138465881\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'haha'\n",
              "2. 0.78786438703537\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'laundry'\n",
              "2. 0.762444496154785\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'hoop'\n",
              "2. 0.762144804000854\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'hallway'\n",
              "2. 0.747283399105072\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'taxi'\n",
              "2. 0.745568156242371\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'slowed'\n",
              "2. 0.743111431598663\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'broom'\n",
              "2. 0.740418314933777\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'latchkey'\n",
              "2. 0.739296555519104\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'joness'\n",
              "2. 0.739260911941528\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'shack'\n",
              "2. 0.738708138465881\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"haha\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7878644\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"laundry\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7624445\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"hoop\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.7621448\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"hallway\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.7472834\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"taxi\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.7455682\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"slowed\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.7431114\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"broom\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.7404183\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"latchkey\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.7392966\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"joness\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.7392609\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"shack\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.7387081\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model$wv$most_similar('motorcar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEKo5UAVh6yM",
        "outputId": "d3ccd6a6-613f-486d-aae8-8b44b899c556"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'france'</li>\n",
              "\t<li>0.726611733436584</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'europe'</li>\n",
              "\t<li>0.703520655632019</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'england'</li>\n",
              "\t<li>0.690242648124695</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'rome'</li>\n",
              "\t<li>0.684619128704071</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'italy'</li>\n",
              "\t<li>0.680796921253204</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'germany'</li>\n",
              "\t<li>0.674243807792664</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'greece'</li>\n",
              "\t<li>0.636934578418732</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'london'</li>\n",
              "\t<li>0.613241791725159</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'america'</li>\n",
              "\t<li>0.5939120054245</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'india'</li>\n",
              "\t<li>0.583802223205566</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'france'\n",
              "\\item 0.726611733436584\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'europe'\n",
              "\\item 0.703520655632019\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'england'\n",
              "\\item 0.690242648124695\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'rome'\n",
              "\\item 0.684619128704071\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'italy'\n",
              "\\item 0.680796921253204\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'germany'\n",
              "\\item 0.674243807792664\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'greece'\n",
              "\\item 0.636934578418732\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'london'\n",
              "\\item 0.613241791725159\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'america'\n",
              "\\item 0.5939120054245\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'india'\n",
              "\\item 0.583802223205566\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'france'\n",
              "2. 0.726611733436584\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'europe'\n",
              "2. 0.703520655632019\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'england'\n",
              "2. 0.690242648124695\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'rome'\n",
              "2. 0.684619128704071\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'italy'\n",
              "2. 0.680796921253204\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'germany'\n",
              "2. 0.674243807792664\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'greece'\n",
              "2. 0.636934578418732\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'london'\n",
              "2. 0.613241791725159\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'america'\n",
              "2. 0.5939120054245\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'india'\n",
              "2. 0.583802223205566\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"france\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7266117\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"europe\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7035207\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"england\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.6902426\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"rome\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.6846191\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"italy\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6807969\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"germany\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6742438\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"greece\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.6369346\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"london\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6132418\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"america\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.593912\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"india\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.5838022\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n",
        "\n",
        "## Q.  What has our model learned about nation-states?\n",
        "\n",
        "\n",
        "model$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2HXJuBsh6yM",
        "outputId": "fcce9cbc-7d2e-455a-b05a-9b8d2ccd9145"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'maiden'</li>\n",
              "\t<li>0.495520412921906</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'louisa'</li>\n",
              "\t<li>0.480717837810516</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'adorable'</li>\n",
              "\t<li>0.478279560804367</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'charms'</li>\n",
              "\t<li>0.46611225605011</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'lover'</li>\n",
              "\t<li>0.466060787439346</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'maid'</li>\n",
              "\t<li>0.44939324259758</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'flora'</li>\n",
              "\t<li>0.447085440158844</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'jane'</li>\n",
              "\t<li>0.447046309709549</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'lucilla'</li>\n",
              "\t<li>0.432486563920975</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'innocent'</li>\n",
              "\t<li>0.431819051504135</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'maiden'\n",
              "\\item 0.495520412921906\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'louisa'\n",
              "\\item 0.480717837810516\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'adorable'\n",
              "\\item 0.478279560804367\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'charms'\n",
              "\\item 0.46611225605011\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'lover'\n",
              "\\item 0.466060787439346\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'maid'\n",
              "\\item 0.44939324259758\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'flora'\n",
              "\\item 0.447085440158844\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'jane'\n",
              "\\item 0.447046309709549\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'lucilla'\n",
              "\\item 0.432486563920975\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'innocent'\n",
              "\\item 0.431819051504135\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'maiden'\n",
              "2. 0.495520412921906\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'louisa'\n",
              "2. 0.480717837810516\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'adorable'\n",
              "2. 0.478279560804367\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'charms'\n",
              "2. 0.46611225605011\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'lover'\n",
              "2. 0.466060787439346\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'maid'\n",
              "2. 0.44939324259758\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'flora'\n",
              "2. 0.447085440158844\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'jane'\n",
              "2. 0.447046309709549\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'lucilla'\n",
              "2. 0.432486563920975\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'innocent'\n",
              "2. 0.431819051504135\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"maiden\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.4955204\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"louisa\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.4807178\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"adorable\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.4782796\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"charms\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.4661123\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"lover\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.4660608\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"maid\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.4493932\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"flora\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.4470854\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"jane\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.4470463\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"lucilla\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.4324866\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"innocent\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.4318191\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## EX. Perform the canonic Word2Vec addition again but leave out a term:\n",
        "##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n",
        "\n",
        "## Q.  What do these indicate semantically?\n",
        "\n",
        "model$wv$most_similar(positive = c('woman'), negative = c('man'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6c1Th0Uh6yM"
      },
      "outputs": [],
      "source": [
        "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
        "##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n",
        "##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n",
        "\n",
        "##  Q. How might we compare word2vec models more generally?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJF2OjUJh6yM",
        "outputId": "2d9b95cd-c784-4ddd-a419-24be0bbe9ecd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'piety'</li>\n",
              "\t<li>0.737276077270508</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'morality'</li>\n",
              "\t<li>0.726690053939819</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'science'</li>\n",
              "\t<li>0.697470963001251</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'prudence'</li>\n",
              "\t<li>0.685539543628693</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'philosophy'</li>\n",
              "\t<li>0.683079183101654</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'wisdom'</li>\n",
              "\t<li>0.651139199733734</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'genius'</li>\n",
              "\t<li>0.650582015514374</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'humanity'</li>\n",
              "\t<li>0.640283465385437</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'modesty'</li>\n",
              "\t<li>0.63694030046463</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'morals'</li>\n",
              "\t<li>0.634059965610504</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'piety'\n",
              "\\item 0.737276077270508\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'morality'\n",
              "\\item 0.726690053939819\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'science'\n",
              "\\item 0.697470963001251\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'prudence'\n",
              "\\item 0.685539543628693\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'philosophy'\n",
              "\\item 0.683079183101654\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'wisdom'\n",
              "\\item 0.651139199733734\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'genius'\n",
              "\\item 0.650582015514374\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'humanity'\n",
              "\\item 0.640283465385437\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'modesty'\n",
              "\\item 0.63694030046463\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'morals'\n",
              "\\item 0.634059965610504\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'piety'\n",
              "2. 0.737276077270508\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'morality'\n",
              "2. 0.726690053939819\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'science'\n",
              "2. 0.697470963001251\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'prudence'\n",
              "2. 0.685539543628693\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'philosophy'\n",
              "2. 0.683079183101654\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'wisdom'\n",
              "2. 0.651139199733734\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'genius'\n",
              "2. 0.650582015514374\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'humanity'\n",
              "2. 0.640283465385437\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'modesty'\n",
              "2. 0.63694030046463\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'morals'\n",
              "2. 0.634059965610504\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"piety\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.7372761\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"morality\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.7266901\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"science\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.697471\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"prudence\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.6855395\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"philosophy\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.6830792\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"wisdom\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.6511392\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"genius\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.650582\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"humanity\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.6402835\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"modesty\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.6369403\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"morals\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.63406\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n",
        "\n",
        "ecco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "c5vgN-WUh6yN",
        "outputId": "a6c5a494-de40-4927-b260-e227864ead67"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><ol>\n",
              "\t<li>'teaching'</li>\n",
              "\t<li>0.597018659114838</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'mathematics'</li>\n",
              "\t<li>0.586554288864136</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'chemistry'</li>\n",
              "\t<li>0.571161866188049</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'poetry'</li>\n",
              "\t<li>0.559655547142029</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'precept'</li>\n",
              "\t<li>0.543889999389648</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'believer'</li>\n",
              "\t<li>0.54315459728241</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'deficient'</li>\n",
              "\t<li>0.540042698383331</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'poetical'</li>\n",
              "\t<li>0.540004074573517</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'virgil'</li>\n",
              "\t<li>0.536787867546082</li>\n",
              "</ol>\n",
              "</li>\n",
              "\t<li><ol>\n",
              "\t<li>'yankee'</li>\n",
              "\t<li>0.529230773448944</li>\n",
              "</ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/latex": [
              "\\begin{enumerate}\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'teaching'\n",
              "\\item 0.597018659114838\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'mathematics'\n",
              "\\item 0.586554288864136\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'chemistry'\n",
              "\\item 0.571161866188049\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'poetry'\n",
              "\\item 0.559655547142029\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'precept'\n",
              "\\item 0.543889999389648\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'believer'\n",
              "\\item 0.54315459728241\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'deficient'\n",
              "\\item 0.540042698383331\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'poetical'\n",
              "\\item 0.540004074573517\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'virgil'\n",
              "\\item 0.536787867546082\n",
              "\\end{enumerate}\n",
              "\n",
              "\\item \\begin{enumerate}\n",
              "\\item 'yankee'\n",
              "\\item 0.529230773448944\n",
              "\\end{enumerate}\n",
              "\n",
              "\\end{enumerate}\n"
            ],
            "text/markdown": [
              "1. 1. 'teaching'\n",
              "2. 0.597018659114838\n",
              "\n",
              "\n",
              "\n",
              "2. 1. 'mathematics'\n",
              "2. 0.586554288864136\n",
              "\n",
              "\n",
              "\n",
              "3. 1. 'chemistry'\n",
              "2. 0.571161866188049\n",
              "\n",
              "\n",
              "\n",
              "4. 1. 'poetry'\n",
              "2. 0.559655547142029\n",
              "\n",
              "\n",
              "\n",
              "5. 1. 'precept'\n",
              "2. 0.543889999389648\n",
              "\n",
              "\n",
              "\n",
              "6. 1. 'believer'\n",
              "2. 0.54315459728241\n",
              "\n",
              "\n",
              "\n",
              "7. 1. 'deficient'\n",
              "2. 0.540042698383331\n",
              "\n",
              "\n",
              "\n",
              "8. 1. 'poetical'\n",
              "2. 0.540004074573517\n",
              "\n",
              "\n",
              "\n",
              "9. 1. 'virgil'\n",
              "2. 0.536787867546082\n",
              "\n",
              "\n",
              "\n",
              "10. 1. 'yankee'\n",
              "2. 0.529230773448944\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "[[1]]\n",
              "[[1]][[1]]\n",
              "[1] \"teaching\"\n",
              "\n",
              "[[1]][[2]]\n",
              "[1] 0.5970187\n",
              "\n",
              "\n",
              "[[2]]\n",
              "[[2]][[1]]\n",
              "[1] \"mathematics\"\n",
              "\n",
              "[[2]][[2]]\n",
              "[1] 0.5865543\n",
              "\n",
              "\n",
              "[[3]]\n",
              "[[3]][[1]]\n",
              "[1] \"chemistry\"\n",
              "\n",
              "[[3]][[2]]\n",
              "[1] 0.5711619\n",
              "\n",
              "\n",
              "[[4]]\n",
              "[[4]][[1]]\n",
              "[1] \"poetry\"\n",
              "\n",
              "[[4]][[2]]\n",
              "[1] 0.5596555\n",
              "\n",
              "\n",
              "[[5]]\n",
              "[[5]][[1]]\n",
              "[1] \"precept\"\n",
              "\n",
              "[[5]][[2]]\n",
              "[1] 0.54389\n",
              "\n",
              "\n",
              "[[6]]\n",
              "[[6]][[1]]\n",
              "[1] \"believer\"\n",
              "\n",
              "[[6]][[2]]\n",
              "[1] 0.5431546\n",
              "\n",
              "\n",
              "[[7]]\n",
              "[[7]][[1]]\n",
              "[1] \"deficient\"\n",
              "\n",
              "[[7]][[2]]\n",
              "[1] 0.5400427\n",
              "\n",
              "\n",
              "[[8]]\n",
              "[[8]][[1]]\n",
              "[1] \"poetical\"\n",
              "\n",
              "[[8]][[2]]\n",
              "[1] 0.5400041\n",
              "\n",
              "\n",
              "[[9]]\n",
              "[[9]][[1]]\n",
              "[1] \"virgil\"\n",
              "\n",
              "[[9]][[2]]\n",
              "[1] 0.5367879\n",
              "\n",
              "\n",
              "[[10]]\n",
              "[[10]][[1]]\n",
              "[1] \"yankee\"\n",
              "\n",
              "[[10]][[2]]\n",
              "[1] 0.5292308\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\n",
        "model$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "nDMIwaZjh6yN"
      },
      "source": [
        "## Concluding Remarks and Resources\n",
        "Throughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec's word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\n",
        "\n",
        "While getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\n",
        "\n",
        "For example:\n",
        "* Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model?\n",
        "* What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society?\n",
        "* How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly?\n",
        "* What might guardrails look like for the safe and equitable management and deployment of language models?\n",
        "\n",
        "## Resources\n",
        "* [UBC Library Generative AI Research Guide](https://guides.library.ubc.ca/GenAI/home)\n",
        "* ... other UBC resources...\n",
        "* [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) by Stephen Wolfram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAa1-WlEh6yN"
      },
      "source": [
        "## References\n",
        "This notebook has been built using the following materials:\n",
        "- Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud\n",
        "- Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n",
        "\n",
        "- [TensorFlow word2vec tutorial](https://www.tensorflow.org/text/tutorials/word2vec)\n",
        "\n",
        "- Anwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\n",
        "\n",
        "- Chandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "4.2.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}